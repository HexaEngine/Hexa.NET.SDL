// ------------------------------------------------------------------------------
// <auto-generated>
//     This code was generated by a tool.
//
//     Changes to this file may cause incorrect behavior and will be lost if
//     the code is regenerated.
// </auto-generated>
// ------------------------------------------------------------------------------

using System;
using System.Runtime.CompilerServices;
using System.Runtime.InteropServices;
using HexaGen.Runtime;

namespace Hexa.NET.SDL3
{
	public unsafe partial class SDL
	{

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(ref SDLGPUDevice device, SDLGPUBuffer* buffer, ref byte text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (byte* ptext = &text)
				{
					SetGPUBufferNameNative((SDLGPUDevice*)pdevice, buffer, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(ref SDLGPUDevice device, SDLGPUBuffer* buffer, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (byte* ptext = text)
				{
					SetGPUBufferNameNative((SDLGPUDevice*)pdevice, buffer, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(ref SDLGPUDevice device, SDLGPUBuffer* buffer, string text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				byte* pStr0 = null;
				int pStrSize0 = 0;
				if (text != null)
				{
					pStrSize0 = Utils.GetByteCountUTF8(text);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
					}
					else
					{
						byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
						pStr0 = pStrStack0;
					}
					int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
					pStr0[pStrOffset0] = 0;
				}
				SetGPUBufferNameNative((SDLGPUDevice*)pdevice, buffer, pStr0);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					Utils.Free(pStr0);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(SDLGPUDevice* device, ref SDLGPUBuffer buffer, ref byte text)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				fixed (byte* ptext = &text)
				{
					SetGPUBufferNameNative(device, (SDLGPUBuffer*)pbuffer, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(SDLGPUDevice* device, ref SDLGPUBuffer buffer, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				fixed (byte* ptext = text)
				{
					SetGPUBufferNameNative(device, (SDLGPUBuffer*)pbuffer, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(SDLGPUDevice* device, ref SDLGPUBuffer buffer, string text)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				byte* pStr0 = null;
				int pStrSize0 = 0;
				if (text != null)
				{
					pStrSize0 = Utils.GetByteCountUTF8(text);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
					}
					else
					{
						byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
						pStr0 = pStrStack0;
					}
					int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
					pStr0[pStrOffset0] = 0;
				}
				SetGPUBufferNameNative(device, (SDLGPUBuffer*)pbuffer, pStr0);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					Utils.Free(pStr0);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(ref SDLGPUDevice device, ref SDLGPUBuffer buffer, ref byte text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					fixed (byte* ptext = &text)
					{
						SetGPUBufferNameNative((SDLGPUDevice*)pdevice, (SDLGPUBuffer*)pbuffer, (byte*)ptext);
					}
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(ref SDLGPUDevice device, ref SDLGPUBuffer buffer, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					fixed (byte* ptext = text)
					{
						SetGPUBufferNameNative((SDLGPUDevice*)pdevice, (SDLGPUBuffer*)pbuffer, (byte*)ptext);
					}
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a buffer.<br/>
		/// You should use SDL_PROP_GPU_BUFFER_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUBuffer instead of this function to avoid thread safety issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// buffer is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBufferName(ref SDLGPUDevice device, ref SDLGPUBuffer buffer, string text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					byte* pStr0 = null;
					int pStrSize0 = 0;
					if (text != null)
					{
						pStrSize0 = Utils.GetByteCountUTF8(text);
						if (pStrSize0 >= Utils.MaxStackallocSize)
						{
							pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
						}
						else
						{
							byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
							pStr0 = pStrStack0;
						}
						int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
						pStr0[pStrOffset0] = 0;
					}
					SetGPUBufferNameNative((SDLGPUDevice*)pdevice, (SDLGPUBuffer*)pbuffer, pStr0);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						Utils.Free(pStr0);
					}
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void SetGPUTextureNameNative(SDLGPUDevice* device, SDLGPUTexture* texture, byte* text)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUTexture*, byte*, void>)funcTable[848])(device, texture, text);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, nint, void>)funcTable[848])((nint)device, (nint)texture, (nint)text);
			#endif
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, SDLGPUTexture* texture, byte* text)
		{
			SetGPUTextureNameNative(device, texture, text);
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, SDLGPUTexture* texture, byte* text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				SetGPUTextureNameNative((SDLGPUDevice*)pdevice, texture, text);
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, ref SDLGPUTexture texture, byte* text)
		{
			fixed (SDLGPUTexture* ptexture = &texture)
			{
				SetGPUTextureNameNative(device, (SDLGPUTexture*)ptexture, text);
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, ref SDLGPUTexture texture, byte* text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTexture* ptexture = &texture)
				{
					SetGPUTextureNameNative((SDLGPUDevice*)pdevice, (SDLGPUTexture*)ptexture, text);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, SDLGPUTexture* texture, ref byte text)
		{
			fixed (byte* ptext = &text)
			{
				SetGPUTextureNameNative(device, texture, (byte*)ptext);
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, SDLGPUTexture* texture, ReadOnlySpan<byte> text)
		{
			fixed (byte* ptext = text)
			{
				SetGPUTextureNameNative(device, texture, (byte*)ptext);
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, SDLGPUTexture* texture, string text)
		{
			byte* pStr0 = null;
			int pStrSize0 = 0;
			if (text != null)
			{
				pStrSize0 = Utils.GetByteCountUTF8(text);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
				}
				else
				{
					byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
					pStr0 = pStrStack0;
				}
				int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
				pStr0[pStrOffset0] = 0;
			}
			SetGPUTextureNameNative(device, texture, pStr0);
			if (pStrSize0 >= Utils.MaxStackallocSize)
			{
				Utils.Free(pStr0);
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, SDLGPUTexture* texture, ref byte text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (byte* ptext = &text)
				{
					SetGPUTextureNameNative((SDLGPUDevice*)pdevice, texture, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, SDLGPUTexture* texture, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (byte* ptext = text)
				{
					SetGPUTextureNameNative((SDLGPUDevice*)pdevice, texture, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, SDLGPUTexture* texture, string text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				byte* pStr0 = null;
				int pStrSize0 = 0;
				if (text != null)
				{
					pStrSize0 = Utils.GetByteCountUTF8(text);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
					}
					else
					{
						byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
						pStr0 = pStrStack0;
					}
					int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
					pStr0[pStrOffset0] = 0;
				}
				SetGPUTextureNameNative((SDLGPUDevice*)pdevice, texture, pStr0);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					Utils.Free(pStr0);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, ref SDLGPUTexture texture, ref byte text)
		{
			fixed (SDLGPUTexture* ptexture = &texture)
			{
				fixed (byte* ptext = &text)
				{
					SetGPUTextureNameNative(device, (SDLGPUTexture*)ptexture, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, ref SDLGPUTexture texture, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUTexture* ptexture = &texture)
			{
				fixed (byte* ptext = text)
				{
					SetGPUTextureNameNative(device, (SDLGPUTexture*)ptexture, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(SDLGPUDevice* device, ref SDLGPUTexture texture, string text)
		{
			fixed (SDLGPUTexture* ptexture = &texture)
			{
				byte* pStr0 = null;
				int pStrSize0 = 0;
				if (text != null)
				{
					pStrSize0 = Utils.GetByteCountUTF8(text);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
					}
					else
					{
						byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
						pStr0 = pStrStack0;
					}
					int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
					pStr0[pStrOffset0] = 0;
				}
				SetGPUTextureNameNative(device, (SDLGPUTexture*)ptexture, pStr0);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					Utils.Free(pStr0);
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, ref SDLGPUTexture texture, ref byte text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTexture* ptexture = &texture)
				{
					fixed (byte* ptext = &text)
					{
						SetGPUTextureNameNative((SDLGPUDevice*)pdevice, (SDLGPUTexture*)ptexture, (byte*)ptext);
					}
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, ref SDLGPUTexture texture, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTexture* ptexture = &texture)
				{
					fixed (byte* ptext = text)
					{
						SetGPUTextureNameNative((SDLGPUDevice*)pdevice, (SDLGPUTexture*)ptexture, (byte*)ptext);
					}
				}
			}
		}

		/// <summary>
		/// Sets an arbitrary string constant to label a texture.<br/>
		/// You should use SDL_PROP_GPU_TEXTURE_CREATE_NAME_STRING with<br/>
		/// SDL_CreateGPUTexture instead of this function to avoid thread safety<br/>
		/// issues.<br/>
		/// <br/>
		/// <br/>
		/// This function is not thread safe, you must make sure the<br/>
		/// texture is not simultaneously used by any other thread.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUTextureName(ref SDLGPUDevice device, ref SDLGPUTexture texture, string text)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTexture* ptexture = &texture)
				{
					byte* pStr0 = null;
					int pStrSize0 = 0;
					if (text != null)
					{
						pStrSize0 = Utils.GetByteCountUTF8(text);
						if (pStrSize0 >= Utils.MaxStackallocSize)
						{
							pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
						}
						else
						{
							byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
							pStr0 = pStrStack0;
						}
						int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
						pStr0[pStrOffset0] = 0;
					}
					SetGPUTextureNameNative((SDLGPUDevice*)pdevice, (SDLGPUTexture*)ptexture, pStr0);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						Utils.Free(pStr0);
					}
				}
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void InsertGPUDebugLabelNative(SDLGPUCommandBuffer* commandBuffer, byte* text)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, byte*, void>)funcTable[849])(commandBuffer, text);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[849])((nint)commandBuffer, (nint)text);
			#endif
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(SDLGPUCommandBuffer* commandBuffer, byte* text)
		{
			InsertGPUDebugLabelNative(commandBuffer, text);
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(ref SDLGPUCommandBuffer commandBuffer, byte* text)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				InsertGPUDebugLabelNative((SDLGPUCommandBuffer*)pcommandBuffer, text);
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(SDLGPUCommandBuffer* commandBuffer, ref byte text)
		{
			fixed (byte* ptext = &text)
			{
				InsertGPUDebugLabelNative(commandBuffer, (byte*)ptext);
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(SDLGPUCommandBuffer* commandBuffer, ReadOnlySpan<byte> text)
		{
			fixed (byte* ptext = text)
			{
				InsertGPUDebugLabelNative(commandBuffer, (byte*)ptext);
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(SDLGPUCommandBuffer* commandBuffer, string text)
		{
			byte* pStr0 = null;
			int pStrSize0 = 0;
			if (text != null)
			{
				pStrSize0 = Utils.GetByteCountUTF8(text);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
				}
				else
				{
					byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
					pStr0 = pStrStack0;
				}
				int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
				pStr0[pStrOffset0] = 0;
			}
			InsertGPUDebugLabelNative(commandBuffer, pStr0);
			if (pStrSize0 >= Utils.MaxStackallocSize)
			{
				Utils.Free(pStr0);
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(ref SDLGPUCommandBuffer commandBuffer, ref byte text)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (byte* ptext = &text)
				{
					InsertGPUDebugLabelNative((SDLGPUCommandBuffer*)pcommandBuffer, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(ref SDLGPUCommandBuffer commandBuffer, ReadOnlySpan<byte> text)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (byte* ptext = text)
				{
					InsertGPUDebugLabelNative((SDLGPUCommandBuffer*)pcommandBuffer, (byte*)ptext);
				}
			}
		}

		/// <summary>
		/// Inserts an arbitrary string label into the command buffer callstream.<br/>
		/// Useful for debugging.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void InsertGPUDebugLabel(ref SDLGPUCommandBuffer commandBuffer, string text)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				byte* pStr0 = null;
				int pStrSize0 = 0;
				if (text != null)
				{
					pStrSize0 = Utils.GetByteCountUTF8(text);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
					}
					else
					{
						byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
						pStr0 = pStrStack0;
					}
					int pStrOffset0 = Utils.EncodeStringUTF8(text, pStr0, pStrSize0);
					pStr0[pStrOffset0] = 0;
				}
				InsertGPUDebugLabelNative((SDLGPUCommandBuffer*)pcommandBuffer, pStr0);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					Utils.Free(pStr0);
				}
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void PushGPUDebugGroupNative(SDLGPUCommandBuffer* commandBuffer, byte* name)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, byte*, void>)funcTable[850])(commandBuffer, name);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[850])((nint)commandBuffer, (nint)name);
			#endif
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(SDLGPUCommandBuffer* commandBuffer, byte* name)
		{
			PushGPUDebugGroupNative(commandBuffer, name);
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(ref SDLGPUCommandBuffer commandBuffer, byte* name)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				PushGPUDebugGroupNative((SDLGPUCommandBuffer*)pcommandBuffer, name);
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(SDLGPUCommandBuffer* commandBuffer, ref byte name)
		{
			fixed (byte* pname = &name)
			{
				PushGPUDebugGroupNative(commandBuffer, (byte*)pname);
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(SDLGPUCommandBuffer* commandBuffer, ReadOnlySpan<byte> name)
		{
			fixed (byte* pname = name)
			{
				PushGPUDebugGroupNative(commandBuffer, (byte*)pname);
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(SDLGPUCommandBuffer* commandBuffer, string name)
		{
			byte* pStr0 = null;
			int pStrSize0 = 0;
			if (name != null)
			{
				pStrSize0 = Utils.GetByteCountUTF8(name);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
				}
				else
				{
					byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
					pStr0 = pStrStack0;
				}
				int pStrOffset0 = Utils.EncodeStringUTF8(name, pStr0, pStrSize0);
				pStr0[pStrOffset0] = 0;
			}
			PushGPUDebugGroupNative(commandBuffer, pStr0);
			if (pStrSize0 >= Utils.MaxStackallocSize)
			{
				Utils.Free(pStr0);
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(ref SDLGPUCommandBuffer commandBuffer, ref byte name)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (byte* pname = &name)
				{
					PushGPUDebugGroupNative((SDLGPUCommandBuffer*)pcommandBuffer, (byte*)pname);
				}
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(ref SDLGPUCommandBuffer commandBuffer, ReadOnlySpan<byte> name)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (byte* pname = name)
				{
					PushGPUDebugGroupNative((SDLGPUCommandBuffer*)pcommandBuffer, (byte*)pname);
				}
			}
		}

		/// <summary>
		/// Begins a debug group with an arbitary name.<br/>
		/// Used for denoting groups of calls when viewing the command buffer<br/>
		/// callstream in a graphics debugging tool.<br/>
		/// Each call to SDL_PushGPUDebugGroup must have a corresponding call to<br/>
		/// SDL_PopGPUDebugGroup.<br/>
		/// On some backends (e.g. Metal), pushing a debug group during a<br/>
		/// render/blit/compute pass will create a group that is scoped to the native<br/>
		/// pass rather than the command buffer. For best results, if you push a debug<br/>
		/// group during a pass, always pop it in the same pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUDebugGroup(ref SDLGPUCommandBuffer commandBuffer, string name)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				byte* pStr0 = null;
				int pStrSize0 = 0;
				if (name != null)
				{
					pStrSize0 = Utils.GetByteCountUTF8(name);
					if (pStrSize0 >= Utils.MaxStackallocSize)
					{
						pStr0 = Utils.Alloc<byte>(pStrSize0 + 1);
					}
					else
					{
						byte* pStrStack0 = stackalloc byte[pStrSize0 + 1];
						pStr0 = pStrStack0;
					}
					int pStrOffset0 = Utils.EncodeStringUTF8(name, pStr0, pStrSize0);
					pStr0[pStrOffset0] = 0;
				}
				PushGPUDebugGroupNative((SDLGPUCommandBuffer*)pcommandBuffer, pStr0);
				if (pStrSize0 >= Utils.MaxStackallocSize)
				{
					Utils.Free(pStr0);
				}
			}
		}

		/// <summary>
		/// Ends the most-recently pushed debug group.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void PopGPUDebugGroupNative(SDLGPUCommandBuffer* commandBuffer)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, void>)funcTable[851])(commandBuffer);
			#else
			((delegate* unmanaged[Cdecl]<nint, void>)funcTable[851])((nint)commandBuffer);
			#endif
		}

		/// <summary>
		/// Ends the most-recently pushed debug group.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PopGPUDebugGroup(SDLGPUCommandBuffer* commandBuffer)
		{
			PopGPUDebugGroupNative(commandBuffer);
		}

		/// <summary>
		/// Ends the most-recently pushed debug group.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PopGPUDebugGroup(ref SDLGPUCommandBuffer commandBuffer)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				PopGPUDebugGroupNative((SDLGPUCommandBuffer*)pcommandBuffer);
			}
		}

		/// <summary>
		/// Frees the given texture as soon as it is safe to do so.<br/>
		/// You must not reference the texture after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUTextureNative(SDLGPUDevice* device, SDLGPUTexture* texture)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUTexture*, void>)funcTable[852])(device, texture);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[852])((nint)device, (nint)texture);
			#endif
		}

		/// <summary>
		/// Frees the given texture as soon as it is safe to do so.<br/>
		/// You must not reference the texture after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTexture(SDLGPUDevice* device, SDLGPUTexture* texture)
		{
			ReleaseGPUTextureNative(device, texture);
		}

		/// <summary>
		/// Frees the given texture as soon as it is safe to do so.<br/>
		/// You must not reference the texture after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTexture(ref SDLGPUDevice device, SDLGPUTexture* texture)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUTextureNative((SDLGPUDevice*)pdevice, texture);
			}
		}

		/// <summary>
		/// Frees the given texture as soon as it is safe to do so.<br/>
		/// You must not reference the texture after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTexture(SDLGPUDevice* device, ref SDLGPUTexture texture)
		{
			fixed (SDLGPUTexture* ptexture = &texture)
			{
				ReleaseGPUTextureNative(device, (SDLGPUTexture*)ptexture);
			}
		}

		/// <summary>
		/// Frees the given texture as soon as it is safe to do so.<br/>
		/// You must not reference the texture after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTexture(ref SDLGPUDevice device, ref SDLGPUTexture texture)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTexture* ptexture = &texture)
				{
					ReleaseGPUTextureNative((SDLGPUDevice*)pdevice, (SDLGPUTexture*)ptexture);
				}
			}
		}

		/// <summary>
		/// Frees the given sampler as soon as it is safe to do so.<br/>
		/// You must not reference the sampler after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUSamplerNative(SDLGPUDevice* device, SDLGPUSampler* sampler)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUSampler*, void>)funcTable[853])(device, sampler);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[853])((nint)device, (nint)sampler);
			#endif
		}

		/// <summary>
		/// Frees the given sampler as soon as it is safe to do so.<br/>
		/// You must not reference the sampler after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUSampler(SDLGPUDevice* device, SDLGPUSampler* sampler)
		{
			ReleaseGPUSamplerNative(device, sampler);
		}

		/// <summary>
		/// Frees the given sampler as soon as it is safe to do so.<br/>
		/// You must not reference the sampler after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUSampler(ref SDLGPUDevice device, SDLGPUSampler* sampler)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUSamplerNative((SDLGPUDevice*)pdevice, sampler);
			}
		}

		/// <summary>
		/// Frees the given sampler as soon as it is safe to do so.<br/>
		/// You must not reference the sampler after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUSampler(SDLGPUDevice* device, ref SDLGPUSampler sampler)
		{
			fixed (SDLGPUSampler* psampler = &sampler)
			{
				ReleaseGPUSamplerNative(device, (SDLGPUSampler*)psampler);
			}
		}

		/// <summary>
		/// Frees the given sampler as soon as it is safe to do so.<br/>
		/// You must not reference the sampler after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUSampler(ref SDLGPUDevice device, ref SDLGPUSampler sampler)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUSampler* psampler = &sampler)
				{
					ReleaseGPUSamplerNative((SDLGPUDevice*)pdevice, (SDLGPUSampler*)psampler);
				}
			}
		}

		/// <summary>
		/// Frees the given buffer as soon as it is safe to do so.<br/>
		/// You must not reference the buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUBufferNative(SDLGPUDevice* device, SDLGPUBuffer* buffer)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUBuffer*, void>)funcTable[854])(device, buffer);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[854])((nint)device, (nint)buffer);
			#endif
		}

		/// <summary>
		/// Frees the given buffer as soon as it is safe to do so.<br/>
		/// You must not reference the buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUBuffer(SDLGPUDevice* device, SDLGPUBuffer* buffer)
		{
			ReleaseGPUBufferNative(device, buffer);
		}

		/// <summary>
		/// Frees the given buffer as soon as it is safe to do so.<br/>
		/// You must not reference the buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUBuffer(ref SDLGPUDevice device, SDLGPUBuffer* buffer)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUBufferNative((SDLGPUDevice*)pdevice, buffer);
			}
		}

		/// <summary>
		/// Frees the given buffer as soon as it is safe to do so.<br/>
		/// You must not reference the buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUBuffer(SDLGPUDevice* device, ref SDLGPUBuffer buffer)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				ReleaseGPUBufferNative(device, (SDLGPUBuffer*)pbuffer);
			}
		}

		/// <summary>
		/// Frees the given buffer as soon as it is safe to do so.<br/>
		/// You must not reference the buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUBuffer(ref SDLGPUDevice device, ref SDLGPUBuffer buffer)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					ReleaseGPUBufferNative((SDLGPUDevice*)pdevice, (SDLGPUBuffer*)pbuffer);
				}
			}
		}

		/// <summary>
		/// Frees the given transfer buffer as soon as it is safe to do so.<br/>
		/// You must not reference the transfer buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUTransferBufferNative(SDLGPUDevice* device, SDLGPUTransferBuffer* transferBuffer)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUTransferBuffer*, void>)funcTable[855])(device, transferBuffer);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[855])((nint)device, (nint)transferBuffer);
			#endif
		}

		/// <summary>
		/// Frees the given transfer buffer as soon as it is safe to do so.<br/>
		/// You must not reference the transfer buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTransferBuffer(SDLGPUDevice* device, SDLGPUTransferBuffer* transferBuffer)
		{
			ReleaseGPUTransferBufferNative(device, transferBuffer);
		}

		/// <summary>
		/// Frees the given transfer buffer as soon as it is safe to do so.<br/>
		/// You must not reference the transfer buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTransferBuffer(ref SDLGPUDevice device, SDLGPUTransferBuffer* transferBuffer)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUTransferBufferNative((SDLGPUDevice*)pdevice, transferBuffer);
			}
		}

		/// <summary>
		/// Frees the given transfer buffer as soon as it is safe to do so.<br/>
		/// You must not reference the transfer buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTransferBuffer(SDLGPUDevice* device, ref SDLGPUTransferBuffer transferBuffer)
		{
			fixed (SDLGPUTransferBuffer* ptransferBuffer = &transferBuffer)
			{
				ReleaseGPUTransferBufferNative(device, (SDLGPUTransferBuffer*)ptransferBuffer);
			}
		}

		/// <summary>
		/// Frees the given transfer buffer as soon as it is safe to do so.<br/>
		/// You must not reference the transfer buffer after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUTransferBuffer(ref SDLGPUDevice device, ref SDLGPUTransferBuffer transferBuffer)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTransferBuffer* ptransferBuffer = &transferBuffer)
				{
					ReleaseGPUTransferBufferNative((SDLGPUDevice*)pdevice, (SDLGPUTransferBuffer*)ptransferBuffer);
				}
			}
		}

		/// <summary>
		/// Frees the given compute pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the compute pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUComputePipelineNative(SDLGPUDevice* device, SDLGPUComputePipeline* computePipeline)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUComputePipeline*, void>)funcTable[856])(device, computePipeline);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[856])((nint)device, (nint)computePipeline);
			#endif
		}

		/// <summary>
		/// Frees the given compute pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the compute pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUComputePipeline(SDLGPUDevice* device, SDLGPUComputePipeline* computePipeline)
		{
			ReleaseGPUComputePipelineNative(device, computePipeline);
		}

		/// <summary>
		/// Frees the given compute pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the compute pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUComputePipeline(ref SDLGPUDevice device, SDLGPUComputePipeline* computePipeline)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUComputePipelineNative((SDLGPUDevice*)pdevice, computePipeline);
			}
		}

		/// <summary>
		/// Frees the given compute pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the compute pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUComputePipeline(SDLGPUDevice* device, ref SDLGPUComputePipeline computePipeline)
		{
			fixed (SDLGPUComputePipeline* pcomputePipeline = &computePipeline)
			{
				ReleaseGPUComputePipelineNative(device, (SDLGPUComputePipeline*)pcomputePipeline);
			}
		}

		/// <summary>
		/// Frees the given compute pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the compute pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUComputePipeline(ref SDLGPUDevice device, ref SDLGPUComputePipeline computePipeline)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUComputePipeline* pcomputePipeline = &computePipeline)
				{
					ReleaseGPUComputePipelineNative((SDLGPUDevice*)pdevice, (SDLGPUComputePipeline*)pcomputePipeline);
				}
			}
		}

		/// <summary>
		/// Frees the given shader as soon as it is safe to do so.<br/>
		/// You must not reference the shader after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUShaderNative(SDLGPUDevice* device, SDLGPUShader* shader)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUShader*, void>)funcTable[857])(device, shader);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[857])((nint)device, (nint)shader);
			#endif
		}

		/// <summary>
		/// Frees the given shader as soon as it is safe to do so.<br/>
		/// You must not reference the shader after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUShader(SDLGPUDevice* device, SDLGPUShader* shader)
		{
			ReleaseGPUShaderNative(device, shader);
		}

		/// <summary>
		/// Frees the given shader as soon as it is safe to do so.<br/>
		/// You must not reference the shader after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUShader(ref SDLGPUDevice device, SDLGPUShader* shader)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUShaderNative((SDLGPUDevice*)pdevice, shader);
			}
		}

		/// <summary>
		/// Frees the given shader as soon as it is safe to do so.<br/>
		/// You must not reference the shader after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUShader(SDLGPUDevice* device, ref SDLGPUShader shader)
		{
			fixed (SDLGPUShader* pshader = &shader)
			{
				ReleaseGPUShaderNative(device, (SDLGPUShader*)pshader);
			}
		}

		/// <summary>
		/// Frees the given shader as soon as it is safe to do so.<br/>
		/// You must not reference the shader after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUShader(ref SDLGPUDevice device, ref SDLGPUShader shader)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUShader* pshader = &shader)
				{
					ReleaseGPUShaderNative((SDLGPUDevice*)pdevice, (SDLGPUShader*)pshader);
				}
			}
		}

		/// <summary>
		/// Frees the given graphics pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the graphics pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void ReleaseGPUGraphicsPipelineNative(SDLGPUDevice* device, SDLGPUGraphicsPipeline* graphicsPipeline)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUGraphicsPipeline*, void>)funcTable[858])(device, graphicsPipeline);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[858])((nint)device, (nint)graphicsPipeline);
			#endif
		}

		/// <summary>
		/// Frees the given graphics pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the graphics pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUGraphicsPipeline(SDLGPUDevice* device, SDLGPUGraphicsPipeline* graphicsPipeline)
		{
			ReleaseGPUGraphicsPipelineNative(device, graphicsPipeline);
		}

		/// <summary>
		/// Frees the given graphics pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the graphics pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUGraphicsPipeline(ref SDLGPUDevice device, SDLGPUGraphicsPipeline* graphicsPipeline)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				ReleaseGPUGraphicsPipelineNative((SDLGPUDevice*)pdevice, graphicsPipeline);
			}
		}

		/// <summary>
		/// Frees the given graphics pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the graphics pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUGraphicsPipeline(SDLGPUDevice* device, ref SDLGPUGraphicsPipeline graphicsPipeline)
		{
			fixed (SDLGPUGraphicsPipeline* pgraphicsPipeline = &graphicsPipeline)
			{
				ReleaseGPUGraphicsPipelineNative(device, (SDLGPUGraphicsPipeline*)pgraphicsPipeline);
			}
		}

		/// <summary>
		/// Frees the given graphics pipeline as soon as it is safe to do so.<br/>
		/// You must not reference the graphics pipeline after calling this function.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void ReleaseGPUGraphicsPipeline(ref SDLGPUDevice device, ref SDLGPUGraphicsPipeline graphicsPipeline)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUGraphicsPipeline* pgraphicsPipeline = &graphicsPipeline)
				{
					ReleaseGPUGraphicsPipelineNative((SDLGPUDevice*)pdevice, (SDLGPUGraphicsPipeline*)pgraphicsPipeline);
				}
			}
		}

		/// <summary>
		/// Acquire a command buffer.<br/>
		/// This command buffer is managed by the implementation and should not be<br/>
		/// freed by the user. The command buffer may only be used on the thread it was<br/>
		/// acquired on. The command buffer should be submitted on the thread it was<br/>
		/// acquired on.<br/>
		/// It is valid to acquire multiple command buffers on the same thread at once.<br/>
		/// In fact a common design pattern is to acquire two command buffers per frame<br/>
		/// where one is dedicated to render and compute passes and the other is<br/>
		/// dedicated to copy passes and other preparatory work such as generating<br/>
		/// mipmaps. Interleaving commands between the two command buffers reduces the<br/>
		/// total amount of passes overall which improves rendering performance.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static SDLGPUCommandBuffer* AcquireGPUCommandBufferNative(SDLGPUDevice* device)
		{
			#if NET5_0_OR_GREATER
			return ((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUCommandBuffer*>)funcTable[859])(device);
			#else
			return (SDLGPUCommandBuffer*)((delegate* unmanaged[Cdecl]<nint, nint>)funcTable[859])((nint)device);
			#endif
		}

		/// <summary>
		/// Acquire a command buffer.<br/>
		/// This command buffer is managed by the implementation and should not be<br/>
		/// freed by the user. The command buffer may only be used on the thread it was<br/>
		/// acquired on. The command buffer should be submitted on the thread it was<br/>
		/// acquired on.<br/>
		/// It is valid to acquire multiple command buffers on the same thread at once.<br/>
		/// In fact a common design pattern is to acquire two command buffers per frame<br/>
		/// where one is dedicated to render and compute passes and the other is<br/>
		/// dedicated to copy passes and other preparatory work such as generating<br/>
		/// mipmaps. Interleaving commands between the two command buffers reduces the<br/>
		/// total amount of passes overall which improves rendering performance.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUCommandBuffer* AcquireGPUCommandBuffer(SDLGPUDevice* device)
		{
			SDLGPUCommandBuffer* ret = AcquireGPUCommandBufferNative(device);
			return ret;
		}

		/// <summary>
		/// Acquire a command buffer.<br/>
		/// This command buffer is managed by the implementation and should not be<br/>
		/// freed by the user. The command buffer may only be used on the thread it was<br/>
		/// acquired on. The command buffer should be submitted on the thread it was<br/>
		/// acquired on.<br/>
		/// It is valid to acquire multiple command buffers on the same thread at once.<br/>
		/// In fact a common design pattern is to acquire two command buffers per frame<br/>
		/// where one is dedicated to render and compute passes and the other is<br/>
		/// dedicated to copy passes and other preparatory work such as generating<br/>
		/// mipmaps. Interleaving commands between the two command buffers reduces the<br/>
		/// total amount of passes overall which improves rendering performance.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUCommandBuffer* AcquireGPUCommandBuffer(ref SDLGPUDevice device)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				SDLGPUCommandBuffer* ret = AcquireGPUCommandBufferNative((SDLGPUDevice*)pdevice);
				return ret;
			}
		}

		/// <summary>
		/// Pushes data to a vertex uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void PushGPUVertexUniformDataNative(SDLGPUCommandBuffer* commandBuffer, uint slotIndex, void* data, uint length)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, uint, void*, uint, void>)funcTable[860])(commandBuffer, slotIndex, data, length);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[860])((nint)commandBuffer, slotIndex, (nint)data, length);
			#endif
		}

		/// <summary>
		/// Pushes data to a vertex uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUVertexUniformData(SDLGPUCommandBuffer* commandBuffer, uint slotIndex, void* data, uint length)
		{
			PushGPUVertexUniformDataNative(commandBuffer, slotIndex, data, length);
		}

		/// <summary>
		/// Pushes data to a vertex uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUVertexUniformData(ref SDLGPUCommandBuffer commandBuffer, uint slotIndex, void* data, uint length)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				PushGPUVertexUniformDataNative((SDLGPUCommandBuffer*)pcommandBuffer, slotIndex, data, length);
			}
		}

		/// <summary>
		/// Pushes data to a fragment uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void PushGPUFragmentUniformDataNative(SDLGPUCommandBuffer* commandBuffer, uint slotIndex, void* data, uint length)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, uint, void*, uint, void>)funcTable[861])(commandBuffer, slotIndex, data, length);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[861])((nint)commandBuffer, slotIndex, (nint)data, length);
			#endif
		}

		/// <summary>
		/// Pushes data to a fragment uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUFragmentUniformData(SDLGPUCommandBuffer* commandBuffer, uint slotIndex, void* data, uint length)
		{
			PushGPUFragmentUniformDataNative(commandBuffer, slotIndex, data, length);
		}

		/// <summary>
		/// Pushes data to a fragment uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUFragmentUniformData(ref SDLGPUCommandBuffer commandBuffer, uint slotIndex, void* data, uint length)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				PushGPUFragmentUniformDataNative((SDLGPUCommandBuffer*)pcommandBuffer, slotIndex, data, length);
			}
		}

		/// <summary>
		/// Pushes data to a uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void PushGPUComputeUniformDataNative(SDLGPUCommandBuffer* commandBuffer, uint slotIndex, void* data, uint length)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, uint, void*, uint, void>)funcTable[862])(commandBuffer, slotIndex, data, length);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[862])((nint)commandBuffer, slotIndex, (nint)data, length);
			#endif
		}

		/// <summary>
		/// Pushes data to a uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUComputeUniformData(SDLGPUCommandBuffer* commandBuffer, uint slotIndex, void* data, uint length)
		{
			PushGPUComputeUniformDataNative(commandBuffer, slotIndex, data, length);
		}

		/// <summary>
		/// Pushes data to a uniform slot on the command buffer.<br/>
		/// Subsequent draw calls will use this uniform data.<br/>
		/// The data being pushed must respect std140 layout conventions. In practical<br/>
		/// terms this means you must ensure that vec3 and vec4 fields are 16-byte<br/>
		/// aligned.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void PushGPUComputeUniformData(ref SDLGPUCommandBuffer commandBuffer, uint slotIndex, void* data, uint length)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				PushGPUComputeUniformDataNative((SDLGPUCommandBuffer*)pcommandBuffer, slotIndex, data, length);
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static SDLGPURenderPass* BeginGPURenderPassNative(SDLGPUCommandBuffer* commandBuffer, SDLGPUColorTargetInfo* colorTargetInfos, uint numColorTargets, SDLGPUDepthStencilTargetInfo* depthStencilTargetInfo)
		{
			#if NET5_0_OR_GREATER
			return ((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, SDLGPUColorTargetInfo*, uint, SDLGPUDepthStencilTargetInfo*, SDLGPURenderPass*>)funcTable[863])(commandBuffer, colorTargetInfos, numColorTargets, depthStencilTargetInfo);
			#else
			return (SDLGPURenderPass*)((delegate* unmanaged[Cdecl]<nint, nint, uint, nint, nint>)funcTable[863])((nint)commandBuffer, (nint)colorTargetInfos, numColorTargets, (nint)depthStencilTargetInfo);
			#endif
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(SDLGPUCommandBuffer* commandBuffer, SDLGPUColorTargetInfo* colorTargetInfos, uint numColorTargets, SDLGPUDepthStencilTargetInfo* depthStencilTargetInfo)
		{
			SDLGPURenderPass* ret = BeginGPURenderPassNative(commandBuffer, colorTargetInfos, numColorTargets, depthStencilTargetInfo);
			return ret;
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(ref SDLGPUCommandBuffer commandBuffer, SDLGPUColorTargetInfo* colorTargetInfos, uint numColorTargets, SDLGPUDepthStencilTargetInfo* depthStencilTargetInfo)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				SDLGPURenderPass* ret = BeginGPURenderPassNative((SDLGPUCommandBuffer*)pcommandBuffer, colorTargetInfos, numColorTargets, depthStencilTargetInfo);
				return ret;
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(SDLGPUCommandBuffer* commandBuffer, ref SDLGPUColorTargetInfo colorTargetInfos, uint numColorTargets, SDLGPUDepthStencilTargetInfo* depthStencilTargetInfo)
		{
			fixed (SDLGPUColorTargetInfo* pcolorTargetInfos = &colorTargetInfos)
			{
				SDLGPURenderPass* ret = BeginGPURenderPassNative(commandBuffer, (SDLGPUColorTargetInfo*)pcolorTargetInfos, numColorTargets, depthStencilTargetInfo);
				return ret;
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(ref SDLGPUCommandBuffer commandBuffer, ref SDLGPUColorTargetInfo colorTargetInfos, uint numColorTargets, SDLGPUDepthStencilTargetInfo* depthStencilTargetInfo)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (SDLGPUColorTargetInfo* pcolorTargetInfos = &colorTargetInfos)
				{
					SDLGPURenderPass* ret = BeginGPURenderPassNative((SDLGPUCommandBuffer*)pcommandBuffer, (SDLGPUColorTargetInfo*)pcolorTargetInfos, numColorTargets, depthStencilTargetInfo);
					return ret;
				}
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(SDLGPUCommandBuffer* commandBuffer, SDLGPUColorTargetInfo* colorTargetInfos, uint numColorTargets, ref SDLGPUDepthStencilTargetInfo depthStencilTargetInfo)
		{
			fixed (SDLGPUDepthStencilTargetInfo* pdepthStencilTargetInfo = &depthStencilTargetInfo)
			{
				SDLGPURenderPass* ret = BeginGPURenderPassNative(commandBuffer, colorTargetInfos, numColorTargets, (SDLGPUDepthStencilTargetInfo*)pdepthStencilTargetInfo);
				return ret;
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(ref SDLGPUCommandBuffer commandBuffer, SDLGPUColorTargetInfo* colorTargetInfos, uint numColorTargets, ref SDLGPUDepthStencilTargetInfo depthStencilTargetInfo)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (SDLGPUDepthStencilTargetInfo* pdepthStencilTargetInfo = &depthStencilTargetInfo)
				{
					SDLGPURenderPass* ret = BeginGPURenderPassNative((SDLGPUCommandBuffer*)pcommandBuffer, colorTargetInfos, numColorTargets, (SDLGPUDepthStencilTargetInfo*)pdepthStencilTargetInfo);
					return ret;
				}
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(SDLGPUCommandBuffer* commandBuffer, ref SDLGPUColorTargetInfo colorTargetInfos, uint numColorTargets, ref SDLGPUDepthStencilTargetInfo depthStencilTargetInfo)
		{
			fixed (SDLGPUColorTargetInfo* pcolorTargetInfos = &colorTargetInfos)
			{
				fixed (SDLGPUDepthStencilTargetInfo* pdepthStencilTargetInfo = &depthStencilTargetInfo)
				{
					SDLGPURenderPass* ret = BeginGPURenderPassNative(commandBuffer, (SDLGPUColorTargetInfo*)pcolorTargetInfos, numColorTargets, (SDLGPUDepthStencilTargetInfo*)pdepthStencilTargetInfo);
					return ret;
				}
			}
		}

		/// <summary>
		/// Begins a render pass on a command buffer.<br/>
		/// A render pass consists of a set of texture subresources (or depth slices in<br/>
		/// the 3D texture case) which will be rendered to during the render pass,<br/>
		/// along with corresponding clear values and load/store operations. All<br/>
		/// operations related to graphics pipelines must take place inside of a render<br/>
		/// pass. A default viewport and scissor state are automatically set when this<br/>
		/// is called. You cannot begin another render pass, or begin a compute pass or<br/>
		/// copy pass until you have ended the render pass.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPURenderPass* BeginGPURenderPass(ref SDLGPUCommandBuffer commandBuffer, ref SDLGPUColorTargetInfo colorTargetInfos, uint numColorTargets, ref SDLGPUDepthStencilTargetInfo depthStencilTargetInfo)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (SDLGPUColorTargetInfo* pcolorTargetInfos = &colorTargetInfos)
				{
					fixed (SDLGPUDepthStencilTargetInfo* pdepthStencilTargetInfo = &depthStencilTargetInfo)
					{
						SDLGPURenderPass* ret = BeginGPURenderPassNative((SDLGPUCommandBuffer*)pcommandBuffer, (SDLGPUColorTargetInfo*)pcolorTargetInfos, numColorTargets, (SDLGPUDepthStencilTargetInfo*)pdepthStencilTargetInfo);
						return ret;
					}
				}
			}
		}

		/// <summary>
		/// Binds a graphics pipeline on a render pass to be used in rendering.<br/>
		/// A graphics pipeline must be bound before making any draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUGraphicsPipelineNative(SDLGPURenderPass* renderPass, SDLGPUGraphicsPipeline* graphicsPipeline)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLGPUGraphicsPipeline*, void>)funcTable[864])(renderPass, graphicsPipeline);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[864])((nint)renderPass, (nint)graphicsPipeline);
			#endif
		}

		/// <summary>
		/// Binds a graphics pipeline on a render pass to be used in rendering.<br/>
		/// A graphics pipeline must be bound before making any draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUGraphicsPipeline(SDLGPURenderPass* renderPass, SDLGPUGraphicsPipeline* graphicsPipeline)
		{
			BindGPUGraphicsPipelineNative(renderPass, graphicsPipeline);
		}

		/// <summary>
		/// Binds a graphics pipeline on a render pass to be used in rendering.<br/>
		/// A graphics pipeline must be bound before making any draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUGraphicsPipeline(ref SDLGPURenderPass renderPass, SDLGPUGraphicsPipeline* graphicsPipeline)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUGraphicsPipelineNative((SDLGPURenderPass*)prenderPass, graphicsPipeline);
			}
		}

		/// <summary>
		/// Binds a graphics pipeline on a render pass to be used in rendering.<br/>
		/// A graphics pipeline must be bound before making any draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUGraphicsPipeline(SDLGPURenderPass* renderPass, ref SDLGPUGraphicsPipeline graphicsPipeline)
		{
			fixed (SDLGPUGraphicsPipeline* pgraphicsPipeline = &graphicsPipeline)
			{
				BindGPUGraphicsPipelineNative(renderPass, (SDLGPUGraphicsPipeline*)pgraphicsPipeline);
			}
		}

		/// <summary>
		/// Binds a graphics pipeline on a render pass to be used in rendering.<br/>
		/// A graphics pipeline must be bound before making any draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUGraphicsPipeline(ref SDLGPURenderPass renderPass, ref SDLGPUGraphicsPipeline graphicsPipeline)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUGraphicsPipeline* pgraphicsPipeline = &graphicsPipeline)
				{
					BindGPUGraphicsPipelineNative((SDLGPURenderPass*)prenderPass, (SDLGPUGraphicsPipeline*)pgraphicsPipeline);
				}
			}
		}

		/// <summary>
		/// Sets the current viewport state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void SetGPUViewportNative(SDLGPURenderPass* renderPass, SDLGPUViewport* viewport)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLGPUViewport*, void>)funcTable[865])(renderPass, viewport);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[865])((nint)renderPass, (nint)viewport);
			#endif
		}

		/// <summary>
		/// Sets the current viewport state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUViewport(SDLGPURenderPass* renderPass, SDLGPUViewport* viewport)
		{
			SetGPUViewportNative(renderPass, viewport);
		}

		/// <summary>
		/// Sets the current viewport state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUViewport(ref SDLGPURenderPass renderPass, SDLGPUViewport* viewport)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				SetGPUViewportNative((SDLGPURenderPass*)prenderPass, viewport);
			}
		}

		/// <summary>
		/// Sets the current viewport state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUViewport(SDLGPURenderPass* renderPass, ref SDLGPUViewport viewport)
		{
			fixed (SDLGPUViewport* pviewport = &viewport)
			{
				SetGPUViewportNative(renderPass, (SDLGPUViewport*)pviewport);
			}
		}

		/// <summary>
		/// Sets the current viewport state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUViewport(ref SDLGPURenderPass renderPass, ref SDLGPUViewport viewport)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUViewport* pviewport = &viewport)
				{
					SetGPUViewportNative((SDLGPURenderPass*)prenderPass, (SDLGPUViewport*)pviewport);
				}
			}
		}

		/// <summary>
		/// Sets the current scissor state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void SetGPUScissorNative(SDLGPURenderPass* renderPass, SDLRect* scissor)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLRect*, void>)funcTable[866])(renderPass, scissor);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[866])((nint)renderPass, (nint)scissor);
			#endif
		}

		/// <summary>
		/// Sets the current scissor state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUScissor(SDLGPURenderPass* renderPass, SDLRect* scissor)
		{
			SetGPUScissorNative(renderPass, scissor);
		}

		/// <summary>
		/// Sets the current scissor state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUScissor(ref SDLGPURenderPass renderPass, SDLRect* scissor)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				SetGPUScissorNative((SDLGPURenderPass*)prenderPass, scissor);
			}
		}

		/// <summary>
		/// Sets the current scissor state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUScissor(SDLGPURenderPass* renderPass, ref SDLRect scissor)
		{
			fixed (SDLRect* pscissor = &scissor)
			{
				SetGPUScissorNative(renderPass, (SDLRect*)pscissor);
			}
		}

		/// <summary>
		/// Sets the current scissor state on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUScissor(ref SDLGPURenderPass renderPass, ref SDLRect scissor)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLRect* pscissor = &scissor)
				{
					SetGPUScissorNative((SDLGPURenderPass*)prenderPass, (SDLRect*)pscissor);
				}
			}
		}

		/// <summary>
		/// Sets the current blend constants on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void SetGPUBlendConstantsNative(SDLGPURenderPass* renderPass, SDLFColor blendConstants)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLFColor, void>)funcTable[867])(renderPass, blendConstants);
			#else
			((delegate* unmanaged[Cdecl]<nint, SDLFColor, void>)funcTable[867])((nint)renderPass, blendConstants);
			#endif
		}

		/// <summary>
		/// Sets the current blend constants on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBlendConstants(SDLGPURenderPass* renderPass, SDLFColor blendConstants)
		{
			SetGPUBlendConstantsNative(renderPass, blendConstants);
		}

		/// <summary>
		/// Sets the current blend constants on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUBlendConstants(ref SDLGPURenderPass renderPass, SDLFColor blendConstants)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				SetGPUBlendConstantsNative((SDLGPURenderPass*)prenderPass, blendConstants);
			}
		}

		/// <summary>
		/// Sets the current stencil reference value on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void SetGPUStencilReferenceNative(SDLGPURenderPass* renderPass, byte reference)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, byte, void>)funcTable[868])(renderPass, reference);
			#else
			((delegate* unmanaged[Cdecl]<nint, byte, void>)funcTable[868])((nint)renderPass, reference);
			#endif
		}

		/// <summary>
		/// Sets the current stencil reference value on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUStencilReference(SDLGPURenderPass* renderPass, byte reference)
		{
			SetGPUStencilReferenceNative(renderPass, reference);
		}

		/// <summary>
		/// Sets the current stencil reference value on a command buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void SetGPUStencilReference(ref SDLGPURenderPass renderPass, byte reference)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				SetGPUStencilReferenceNative((SDLGPURenderPass*)prenderPass, reference);
			}
		}

		/// <summary>
		/// Binds vertex buffers on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUVertexBuffersNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUBufferBinding* bindings, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUBufferBinding*, uint, void>)funcTable[869])(renderPass, firstSlot, bindings, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[869])((nint)renderPass, firstSlot, (nint)bindings, numBindings);
			#endif
		}

		/// <summary>
		/// Binds vertex buffers on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexBuffers(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUBufferBinding* bindings, uint numBindings)
		{
			BindGPUVertexBuffersNative(renderPass, firstSlot, bindings, numBindings);
		}

		/// <summary>
		/// Binds vertex buffers on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexBuffers(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUBufferBinding* bindings, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUVertexBuffersNative((SDLGPURenderPass*)prenderPass, firstSlot, bindings, numBindings);
			}
		}

		/// <summary>
		/// Binds vertex buffers on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexBuffers(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUBufferBinding bindings, uint numBindings)
		{
			fixed (SDLGPUBufferBinding* pbindings = &bindings)
			{
				BindGPUVertexBuffersNative(renderPass, firstSlot, (SDLGPUBufferBinding*)pbindings, numBindings);
			}
		}

		/// <summary>
		/// Binds vertex buffers on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexBuffers(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUBufferBinding bindings, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUBufferBinding* pbindings = &bindings)
				{
					BindGPUVertexBuffersNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUBufferBinding*)pbindings, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds an index buffer on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUIndexBufferNative(SDLGPURenderPass* renderPass, SDLGPUBufferBinding* binding, SDLGPUIndexElementSize indexElementSize)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLGPUBufferBinding*, SDLGPUIndexElementSize, void>)funcTable[870])(renderPass, binding, indexElementSize);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, SDLGPUIndexElementSize, void>)funcTable[870])((nint)renderPass, (nint)binding, indexElementSize);
			#endif
		}

		/// <summary>
		/// Binds an index buffer on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUIndexBuffer(SDLGPURenderPass* renderPass, SDLGPUBufferBinding* binding, SDLGPUIndexElementSize indexElementSize)
		{
			BindGPUIndexBufferNative(renderPass, binding, indexElementSize);
		}

		/// <summary>
		/// Binds an index buffer on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUIndexBuffer(ref SDLGPURenderPass renderPass, SDLGPUBufferBinding* binding, SDLGPUIndexElementSize indexElementSize)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUIndexBufferNative((SDLGPURenderPass*)prenderPass, binding, indexElementSize);
			}
		}

		/// <summary>
		/// Binds an index buffer on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUIndexBuffer(SDLGPURenderPass* renderPass, ref SDLGPUBufferBinding binding, SDLGPUIndexElementSize indexElementSize)
		{
			fixed (SDLGPUBufferBinding* pbinding = &binding)
			{
				BindGPUIndexBufferNative(renderPass, (SDLGPUBufferBinding*)pbinding, indexElementSize);
			}
		}

		/// <summary>
		/// Binds an index buffer on a command buffer for use with subsequent draw<br/>
		/// calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUIndexBuffer(ref SDLGPURenderPass renderPass, ref SDLGPUBufferBinding binding, SDLGPUIndexElementSize indexElementSize)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUBufferBinding* pbinding = &binding)
				{
					BindGPUIndexBufferNative((SDLGPURenderPass*)prenderPass, (SDLGPUBufferBinding*)pbinding, indexElementSize);
				}
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the vertex shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUVertexSamplersNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUTextureSamplerBinding*, uint, void>)funcTable[871])(renderPass, firstSlot, textureSamplerBindings, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[871])((nint)renderPass, firstSlot, (nint)textureSamplerBindings, numBindings);
			#endif
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the vertex shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexSamplers(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			BindGPUVertexSamplersNative(renderPass, firstSlot, textureSamplerBindings, numBindings);
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the vertex shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexSamplers(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUVertexSamplersNative((SDLGPURenderPass*)prenderPass, firstSlot, textureSamplerBindings, numBindings);
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the vertex shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexSamplers(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUTextureSamplerBinding textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPUTextureSamplerBinding* ptextureSamplerBindings = &textureSamplerBindings)
			{
				BindGPUVertexSamplersNative(renderPass, firstSlot, (SDLGPUTextureSamplerBinding*)ptextureSamplerBindings, numBindings);
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the vertex shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexSamplers(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUTextureSamplerBinding textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUTextureSamplerBinding* ptextureSamplerBindings = &textureSamplerBindings)
				{
					BindGPUVertexSamplersNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUTextureSamplerBinding*)ptextureSamplerBindings, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds storage textures for use on the vertex shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUVertexStorageTexturesNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUTexture**, uint, void>)funcTable[872])(renderPass, firstSlot, storageTextures, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[872])((nint)renderPass, firstSlot, (nint)storageTextures, numBindings);
			#endif
		}

		/// <summary>
		/// Binds storage textures for use on the vertex shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageTextures(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			BindGPUVertexStorageTexturesNative(renderPass, firstSlot, storageTextures, numBindings);
		}

		/// <summary>
		/// Binds storage textures for use on the vertex shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageTextures(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUVertexStorageTexturesNative((SDLGPURenderPass*)prenderPass, firstSlot, storageTextures, numBindings);
			}
		}

		/// <summary>
		/// Binds storage textures for use on the vertex shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageTextures(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUTexture* storageTextures, uint numBindings)
		{
			fixed (SDLGPUTexture** pstorageTextures = &storageTextures)
			{
				BindGPUVertexStorageTexturesNative(renderPass, firstSlot, (SDLGPUTexture**)pstorageTextures, numBindings);
			}
		}

		/// <summary>
		/// Binds storage textures for use on the vertex shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageTextures(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUTexture* storageTextures, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUTexture** pstorageTextures = &storageTextures)
				{
					BindGPUVertexStorageTexturesNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUTexture**)pstorageTextures, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds storage buffers for use on the vertex shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUVertexStorageBuffersNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUBuffer**, uint, void>)funcTable[873])(renderPass, firstSlot, storageBuffers, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[873])((nint)renderPass, firstSlot, (nint)storageBuffers, numBindings);
			#endif
		}

		/// <summary>
		/// Binds storage buffers for use on the vertex shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageBuffers(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			BindGPUVertexStorageBuffersNative(renderPass, firstSlot, storageBuffers, numBindings);
		}

		/// <summary>
		/// Binds storage buffers for use on the vertex shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageBuffers(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUVertexStorageBuffersNative((SDLGPURenderPass*)prenderPass, firstSlot, storageBuffers, numBindings);
			}
		}

		/// <summary>
		/// Binds storage buffers for use on the vertex shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageBuffers(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUBuffer* storageBuffers, uint numBindings)
		{
			fixed (SDLGPUBuffer** pstorageBuffers = &storageBuffers)
			{
				BindGPUVertexStorageBuffersNative(renderPass, firstSlot, (SDLGPUBuffer**)pstorageBuffers, numBindings);
			}
		}

		/// <summary>
		/// Binds storage buffers for use on the vertex shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUVertexStorageBuffers(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUBuffer* storageBuffers, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUBuffer** pstorageBuffers = &storageBuffers)
				{
					BindGPUVertexStorageBuffersNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUBuffer**)pstorageBuffers, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the fragment shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUFragmentSamplersNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUTextureSamplerBinding*, uint, void>)funcTable[874])(renderPass, firstSlot, textureSamplerBindings, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[874])((nint)renderPass, firstSlot, (nint)textureSamplerBindings, numBindings);
			#endif
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the fragment shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentSamplers(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			BindGPUFragmentSamplersNative(renderPass, firstSlot, textureSamplerBindings, numBindings);
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the fragment shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentSamplers(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUFragmentSamplersNative((SDLGPURenderPass*)prenderPass, firstSlot, textureSamplerBindings, numBindings);
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the fragment shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentSamplers(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUTextureSamplerBinding textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPUTextureSamplerBinding* ptextureSamplerBindings = &textureSamplerBindings)
			{
				BindGPUFragmentSamplersNative(renderPass, firstSlot, (SDLGPUTextureSamplerBinding*)ptextureSamplerBindings, numBindings);
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the fragment shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentSamplers(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUTextureSamplerBinding textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUTextureSamplerBinding* ptextureSamplerBindings = &textureSamplerBindings)
				{
					BindGPUFragmentSamplersNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUTextureSamplerBinding*)ptextureSamplerBindings, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds storage textures for use on the fragment shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUFragmentStorageTexturesNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUTexture**, uint, void>)funcTable[875])(renderPass, firstSlot, storageTextures, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[875])((nint)renderPass, firstSlot, (nint)storageTextures, numBindings);
			#endif
		}

		/// <summary>
		/// Binds storage textures for use on the fragment shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageTextures(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			BindGPUFragmentStorageTexturesNative(renderPass, firstSlot, storageTextures, numBindings);
		}

		/// <summary>
		/// Binds storage textures for use on the fragment shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageTextures(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUFragmentStorageTexturesNative((SDLGPURenderPass*)prenderPass, firstSlot, storageTextures, numBindings);
			}
		}

		/// <summary>
		/// Binds storage textures for use on the fragment shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageTextures(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUTexture* storageTextures, uint numBindings)
		{
			fixed (SDLGPUTexture** pstorageTextures = &storageTextures)
			{
				BindGPUFragmentStorageTexturesNative(renderPass, firstSlot, (SDLGPUTexture**)pstorageTextures, numBindings);
			}
		}

		/// <summary>
		/// Binds storage textures for use on the fragment shader.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageTextures(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUTexture* storageTextures, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUTexture** pstorageTextures = &storageTextures)
				{
					BindGPUFragmentStorageTexturesNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUTexture**)pstorageTextures, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds storage buffers for use on the fragment shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUFragmentStorageBuffersNative(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, SDLGPUBuffer**, uint, void>)funcTable[876])(renderPass, firstSlot, storageBuffers, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[876])((nint)renderPass, firstSlot, (nint)storageBuffers, numBindings);
			#endif
		}

		/// <summary>
		/// Binds storage buffers for use on the fragment shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageBuffers(SDLGPURenderPass* renderPass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			BindGPUFragmentStorageBuffersNative(renderPass, firstSlot, storageBuffers, numBindings);
		}

		/// <summary>
		/// Binds storage buffers for use on the fragment shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageBuffers(ref SDLGPURenderPass renderPass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				BindGPUFragmentStorageBuffersNative((SDLGPURenderPass*)prenderPass, firstSlot, storageBuffers, numBindings);
			}
		}

		/// <summary>
		/// Binds storage buffers for use on the fragment shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageBuffers(SDLGPURenderPass* renderPass, uint firstSlot, ref SDLGPUBuffer* storageBuffers, uint numBindings)
		{
			fixed (SDLGPUBuffer** pstorageBuffers = &storageBuffers)
			{
				BindGPUFragmentStorageBuffersNative(renderPass, firstSlot, (SDLGPUBuffer**)pstorageBuffers, numBindings);
			}
		}

		/// <summary>
		/// Binds storage buffers for use on the fragment shader.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_GRAPHICS_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUFragmentStorageBuffers(ref SDLGPURenderPass renderPass, uint firstSlot, ref SDLGPUBuffer* storageBuffers, uint numBindings)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUBuffer** pstorageBuffers = &storageBuffers)
				{
					BindGPUFragmentStorageBuffersNative((SDLGPURenderPass*)prenderPass, firstSlot, (SDLGPUBuffer**)pstorageBuffers, numBindings);
				}
			}
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer and instancing<br/>
		/// enabled.<br/>
		/// You must not call this function before binding a graphics pipeline.<br/>
		/// Note that the `first_vertex` and `first_instance` parameters are NOT<br/>
		/// compatible with built-in vertex/instance ID variables in shaders (for<br/>
		/// example, SV_VertexID); GPU APIs and shader languages do not define these<br/>
		/// built-in variables consistently, so if your shader depends on them, the<br/>
		/// only way to keep behavior consistent and portable is to always pass 0 for<br/>
		/// the correlating parameter in the draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void DrawGPUIndexedPrimitivesNative(SDLGPURenderPass* renderPass, uint numIndices, uint numInstances, uint firstIndex, int vertexOffset, uint firstInstance)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, uint, uint, int, uint, void>)funcTable[877])(renderPass, numIndices, numInstances, firstIndex, vertexOffset, firstInstance);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, uint, uint, int, uint, void>)funcTable[877])((nint)renderPass, numIndices, numInstances, firstIndex, vertexOffset, firstInstance);
			#endif
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer and instancing<br/>
		/// enabled.<br/>
		/// You must not call this function before binding a graphics pipeline.<br/>
		/// Note that the `first_vertex` and `first_instance` parameters are NOT<br/>
		/// compatible with built-in vertex/instance ID variables in shaders (for<br/>
		/// example, SV_VertexID); GPU APIs and shader languages do not define these<br/>
		/// built-in variables consistently, so if your shader depends on them, the<br/>
		/// only way to keep behavior consistent and portable is to always pass 0 for<br/>
		/// the correlating parameter in the draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUIndexedPrimitives(SDLGPURenderPass* renderPass, uint numIndices, uint numInstances, uint firstIndex, int vertexOffset, uint firstInstance)
		{
			DrawGPUIndexedPrimitivesNative(renderPass, numIndices, numInstances, firstIndex, vertexOffset, firstInstance);
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer and instancing<br/>
		/// enabled.<br/>
		/// You must not call this function before binding a graphics pipeline.<br/>
		/// Note that the `first_vertex` and `first_instance` parameters are NOT<br/>
		/// compatible with built-in vertex/instance ID variables in shaders (for<br/>
		/// example, SV_VertexID); GPU APIs and shader languages do not define these<br/>
		/// built-in variables consistently, so if your shader depends on them, the<br/>
		/// only way to keep behavior consistent and portable is to always pass 0 for<br/>
		/// the correlating parameter in the draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUIndexedPrimitives(ref SDLGPURenderPass renderPass, uint numIndices, uint numInstances, uint firstIndex, int vertexOffset, uint firstInstance)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				DrawGPUIndexedPrimitivesNative((SDLGPURenderPass*)prenderPass, numIndices, numInstances, firstIndex, vertexOffset, firstInstance);
			}
		}

		/// <summary>
		/// Draws data using bound graphics state.<br/>
		/// You must not call this function before binding a graphics pipeline.<br/>
		/// Note that the `first_vertex` and `first_instance` parameters are NOT<br/>
		/// compatible with built-in vertex/instance ID variables in shaders (for<br/>
		/// example, SV_VertexID); GPU APIs and shader languages do not define these<br/>
		/// built-in variables consistently, so if your shader depends on them, the<br/>
		/// only way to keep behavior consistent and portable is to always pass 0 for<br/>
		/// the correlating parameter in the draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void DrawGPUPrimitivesNative(SDLGPURenderPass* renderPass, uint numVertices, uint numInstances, uint firstVertex, uint firstInstance)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, uint, uint, uint, uint, void>)funcTable[878])(renderPass, numVertices, numInstances, firstVertex, firstInstance);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, uint, uint, uint, void>)funcTable[878])((nint)renderPass, numVertices, numInstances, firstVertex, firstInstance);
			#endif
		}

		/// <summary>
		/// Draws data using bound graphics state.<br/>
		/// You must not call this function before binding a graphics pipeline.<br/>
		/// Note that the `first_vertex` and `first_instance` parameters are NOT<br/>
		/// compatible with built-in vertex/instance ID variables in shaders (for<br/>
		/// example, SV_VertexID); GPU APIs and shader languages do not define these<br/>
		/// built-in variables consistently, so if your shader depends on them, the<br/>
		/// only way to keep behavior consistent and portable is to always pass 0 for<br/>
		/// the correlating parameter in the draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUPrimitives(SDLGPURenderPass* renderPass, uint numVertices, uint numInstances, uint firstVertex, uint firstInstance)
		{
			DrawGPUPrimitivesNative(renderPass, numVertices, numInstances, firstVertex, firstInstance);
		}

		/// <summary>
		/// Draws data using bound graphics state.<br/>
		/// You must not call this function before binding a graphics pipeline.<br/>
		/// Note that the `first_vertex` and `first_instance` parameters are NOT<br/>
		/// compatible with built-in vertex/instance ID variables in shaders (for<br/>
		/// example, SV_VertexID); GPU APIs and shader languages do not define these<br/>
		/// built-in variables consistently, so if your shader depends on them, the<br/>
		/// only way to keep behavior consistent and portable is to always pass 0 for<br/>
		/// the correlating parameter in the draw calls.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUPrimitives(ref SDLGPURenderPass renderPass, uint numVertices, uint numInstances, uint firstVertex, uint firstInstance)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				DrawGPUPrimitivesNative((SDLGPURenderPass*)prenderPass, numVertices, numInstances, firstVertex, firstInstance);
			}
		}

		/// <summary>
		/// Draws data using bound graphics state and with draw parameters set from a<br/>
		/// buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndirectDrawCommand. You must not call this<br/>
		/// function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void DrawGPUPrimitivesIndirectNative(SDLGPURenderPass* renderPass, SDLGPUBuffer* buffer, uint offset, uint drawCount)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLGPUBuffer*, uint, uint, void>)funcTable[879])(renderPass, buffer, offset, drawCount);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, uint, uint, void>)funcTable[879])((nint)renderPass, (nint)buffer, offset, drawCount);
			#endif
		}

		/// <summary>
		/// Draws data using bound graphics state and with draw parameters set from a<br/>
		/// buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndirectDrawCommand. You must not call this<br/>
		/// function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUPrimitivesIndirect(SDLGPURenderPass* renderPass, SDLGPUBuffer* buffer, uint offset, uint drawCount)
		{
			DrawGPUPrimitivesIndirectNative(renderPass, buffer, offset, drawCount);
		}

		/// <summary>
		/// Draws data using bound graphics state and with draw parameters set from a<br/>
		/// buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndirectDrawCommand. You must not call this<br/>
		/// function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUPrimitivesIndirect(ref SDLGPURenderPass renderPass, SDLGPUBuffer* buffer, uint offset, uint drawCount)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				DrawGPUPrimitivesIndirectNative((SDLGPURenderPass*)prenderPass, buffer, offset, drawCount);
			}
		}

		/// <summary>
		/// Draws data using bound graphics state and with draw parameters set from a<br/>
		/// buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndirectDrawCommand. You must not call this<br/>
		/// function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUPrimitivesIndirect(SDLGPURenderPass* renderPass, ref SDLGPUBuffer buffer, uint offset, uint drawCount)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				DrawGPUPrimitivesIndirectNative(renderPass, (SDLGPUBuffer*)pbuffer, offset, drawCount);
			}
		}

		/// <summary>
		/// Draws data using bound graphics state and with draw parameters set from a<br/>
		/// buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndirectDrawCommand. You must not call this<br/>
		/// function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUPrimitivesIndirect(ref SDLGPURenderPass renderPass, ref SDLGPUBuffer buffer, uint offset, uint drawCount)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					DrawGPUPrimitivesIndirectNative((SDLGPURenderPass*)prenderPass, (SDLGPUBuffer*)pbuffer, offset, drawCount);
				}
			}
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer enabled and with<br/>
		/// draw parameters set from a buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndexedIndirectDrawCommand. You must not call<br/>
		/// this function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void DrawGPUIndexedPrimitivesIndirectNative(SDLGPURenderPass* renderPass, SDLGPUBuffer* buffer, uint offset, uint drawCount)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, SDLGPUBuffer*, uint, uint, void>)funcTable[880])(renderPass, buffer, offset, drawCount);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, uint, uint, void>)funcTable[880])((nint)renderPass, (nint)buffer, offset, drawCount);
			#endif
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer enabled and with<br/>
		/// draw parameters set from a buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndexedIndirectDrawCommand. You must not call<br/>
		/// this function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUIndexedPrimitivesIndirect(SDLGPURenderPass* renderPass, SDLGPUBuffer* buffer, uint offset, uint drawCount)
		{
			DrawGPUIndexedPrimitivesIndirectNative(renderPass, buffer, offset, drawCount);
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer enabled and with<br/>
		/// draw parameters set from a buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndexedIndirectDrawCommand. You must not call<br/>
		/// this function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUIndexedPrimitivesIndirect(ref SDLGPURenderPass renderPass, SDLGPUBuffer* buffer, uint offset, uint drawCount)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				DrawGPUIndexedPrimitivesIndirectNative((SDLGPURenderPass*)prenderPass, buffer, offset, drawCount);
			}
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer enabled and with<br/>
		/// draw parameters set from a buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndexedIndirectDrawCommand. You must not call<br/>
		/// this function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUIndexedPrimitivesIndirect(SDLGPURenderPass* renderPass, ref SDLGPUBuffer buffer, uint offset, uint drawCount)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				DrawGPUIndexedPrimitivesIndirectNative(renderPass, (SDLGPUBuffer*)pbuffer, offset, drawCount);
			}
		}

		/// <summary>
		/// Draws data using bound graphics state with an index buffer enabled and with<br/>
		/// draw parameters set from a buffer.<br/>
		/// The buffer must consist of tightly-packed draw parameter sets that each<br/>
		/// match the layout of SDL_GPUIndexedIndirectDrawCommand. You must not call<br/>
		/// this function before binding a graphics pipeline.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DrawGPUIndexedPrimitivesIndirect(ref SDLGPURenderPass renderPass, ref SDLGPUBuffer buffer, uint offset, uint drawCount)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					DrawGPUIndexedPrimitivesIndirectNative((SDLGPURenderPass*)prenderPass, (SDLGPUBuffer*)pbuffer, offset, drawCount);
				}
			}
		}

		/// <summary>
		/// Ends the given render pass.<br/>
		/// All bound graphics state on the render pass command buffer is unset. The<br/>
		/// render pass handle is now invalid.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void EndGPURenderPassNative(SDLGPURenderPass* renderPass)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPURenderPass*, void>)funcTable[881])(renderPass);
			#else
			((delegate* unmanaged[Cdecl]<nint, void>)funcTable[881])((nint)renderPass);
			#endif
		}

		/// <summary>
		/// Ends the given render pass.<br/>
		/// All bound graphics state on the render pass command buffer is unset. The<br/>
		/// render pass handle is now invalid.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void EndGPURenderPass(SDLGPURenderPass* renderPass)
		{
			EndGPURenderPassNative(renderPass);
		}

		/// <summary>
		/// Ends the given render pass.<br/>
		/// All bound graphics state on the render pass command buffer is unset. The<br/>
		/// render pass handle is now invalid.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void EndGPURenderPass(ref SDLGPURenderPass renderPass)
		{
			fixed (SDLGPURenderPass* prenderPass = &renderPass)
			{
				EndGPURenderPassNative((SDLGPURenderPass*)prenderPass);
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static SDLGPUComputePass* BeginGPUComputePassNative(SDLGPUCommandBuffer* commandBuffer, SDLGPUStorageTextureReadWriteBinding* storageTextureBindings, uint numStorageTextureBindings, SDLGPUStorageBufferReadWriteBinding* storageBufferBindings, uint numStorageBufferBindings)
		{
			#if NET5_0_OR_GREATER
			return ((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, SDLGPUStorageTextureReadWriteBinding*, uint, SDLGPUStorageBufferReadWriteBinding*, uint, SDLGPUComputePass*>)funcTable[882])(commandBuffer, storageTextureBindings, numStorageTextureBindings, storageBufferBindings, numStorageBufferBindings);
			#else
			return (SDLGPUComputePass*)((delegate* unmanaged[Cdecl]<nint, nint, uint, nint, uint, nint>)funcTable[882])((nint)commandBuffer, (nint)storageTextureBindings, numStorageTextureBindings, (nint)storageBufferBindings, numStorageBufferBindings);
			#endif
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(SDLGPUCommandBuffer* commandBuffer, SDLGPUStorageTextureReadWriteBinding* storageTextureBindings, uint numStorageTextureBindings, SDLGPUStorageBufferReadWriteBinding* storageBufferBindings, uint numStorageBufferBindings)
		{
			SDLGPUComputePass* ret = BeginGPUComputePassNative(commandBuffer, storageTextureBindings, numStorageTextureBindings, storageBufferBindings, numStorageBufferBindings);
			return ret;
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(ref SDLGPUCommandBuffer commandBuffer, SDLGPUStorageTextureReadWriteBinding* storageTextureBindings, uint numStorageTextureBindings, SDLGPUStorageBufferReadWriteBinding* storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				SDLGPUComputePass* ret = BeginGPUComputePassNative((SDLGPUCommandBuffer*)pcommandBuffer, storageTextureBindings, numStorageTextureBindings, storageBufferBindings, numStorageBufferBindings);
				return ret;
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(SDLGPUCommandBuffer* commandBuffer, ref SDLGPUStorageTextureReadWriteBinding storageTextureBindings, uint numStorageTextureBindings, SDLGPUStorageBufferReadWriteBinding* storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUStorageTextureReadWriteBinding* pstorageTextureBindings = &storageTextureBindings)
			{
				SDLGPUComputePass* ret = BeginGPUComputePassNative(commandBuffer, (SDLGPUStorageTextureReadWriteBinding*)pstorageTextureBindings, numStorageTextureBindings, storageBufferBindings, numStorageBufferBindings);
				return ret;
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(ref SDLGPUCommandBuffer commandBuffer, ref SDLGPUStorageTextureReadWriteBinding storageTextureBindings, uint numStorageTextureBindings, SDLGPUStorageBufferReadWriteBinding* storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (SDLGPUStorageTextureReadWriteBinding* pstorageTextureBindings = &storageTextureBindings)
				{
					SDLGPUComputePass* ret = BeginGPUComputePassNative((SDLGPUCommandBuffer*)pcommandBuffer, (SDLGPUStorageTextureReadWriteBinding*)pstorageTextureBindings, numStorageTextureBindings, storageBufferBindings, numStorageBufferBindings);
					return ret;
				}
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(SDLGPUCommandBuffer* commandBuffer, SDLGPUStorageTextureReadWriteBinding* storageTextureBindings, uint numStorageTextureBindings, ref SDLGPUStorageBufferReadWriteBinding storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUStorageBufferReadWriteBinding* pstorageBufferBindings = &storageBufferBindings)
			{
				SDLGPUComputePass* ret = BeginGPUComputePassNative(commandBuffer, storageTextureBindings, numStorageTextureBindings, (SDLGPUStorageBufferReadWriteBinding*)pstorageBufferBindings, numStorageBufferBindings);
				return ret;
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(ref SDLGPUCommandBuffer commandBuffer, SDLGPUStorageTextureReadWriteBinding* storageTextureBindings, uint numStorageTextureBindings, ref SDLGPUStorageBufferReadWriteBinding storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (SDLGPUStorageBufferReadWriteBinding* pstorageBufferBindings = &storageBufferBindings)
				{
					SDLGPUComputePass* ret = BeginGPUComputePassNative((SDLGPUCommandBuffer*)pcommandBuffer, storageTextureBindings, numStorageTextureBindings, (SDLGPUStorageBufferReadWriteBinding*)pstorageBufferBindings, numStorageBufferBindings);
					return ret;
				}
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(SDLGPUCommandBuffer* commandBuffer, ref SDLGPUStorageTextureReadWriteBinding storageTextureBindings, uint numStorageTextureBindings, ref SDLGPUStorageBufferReadWriteBinding storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUStorageTextureReadWriteBinding* pstorageTextureBindings = &storageTextureBindings)
			{
				fixed (SDLGPUStorageBufferReadWriteBinding* pstorageBufferBindings = &storageBufferBindings)
				{
					SDLGPUComputePass* ret = BeginGPUComputePassNative(commandBuffer, (SDLGPUStorageTextureReadWriteBinding*)pstorageTextureBindings, numStorageTextureBindings, (SDLGPUStorageBufferReadWriteBinding*)pstorageBufferBindings, numStorageBufferBindings);
					return ret;
				}
			}
		}

		/// <summary>
		/// Begins a compute pass on a command buffer.<br/>
		/// A compute pass is defined by a set of texture subresources and buffers that<br/>
		/// may be written to by compute pipelines. These textures and buffers must<br/>
		/// have been created with the COMPUTE_STORAGE_WRITE bit or the<br/>
		/// COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE bit. If you do not create a texture<br/>
		/// with COMPUTE_STORAGE_SIMULTANEOUS_READ_WRITE, you must not read from the<br/>
		/// texture in the compute pass. All operations related to compute pipelines<br/>
		/// must take place inside of a compute pass. You must not begin another<br/>
		/// compute pass, or a render pass or copy pass before ending the compute pass.<br/>
		/// A VERY IMPORTANT NOTE - Reads and writes in compute passes are NOT<br/>
		/// implicitly synchronized. This means you may cause data races by both<br/>
		/// reading and writing a resource region in a compute pass, or by writing<br/>
		/// multiple times to a resource region. If your compute work depends on<br/>
		/// reading the completed output from a previous dispatch, you MUST end the<br/>
		/// current compute pass and begin a new one before you can safely access the<br/>
		/// data. Otherwise you will receive unexpected results. Reading and writing a<br/>
		/// texture in the same compute pass is only supported by specific texture<br/>
		/// formats. Make sure you check the format support!<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUComputePass* BeginGPUComputePass(ref SDLGPUCommandBuffer commandBuffer, ref SDLGPUStorageTextureReadWriteBinding storageTextureBindings, uint numStorageTextureBindings, ref SDLGPUStorageBufferReadWriteBinding storageBufferBindings, uint numStorageBufferBindings)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				fixed (SDLGPUStorageTextureReadWriteBinding* pstorageTextureBindings = &storageTextureBindings)
				{
					fixed (SDLGPUStorageBufferReadWriteBinding* pstorageBufferBindings = &storageBufferBindings)
					{
						SDLGPUComputePass* ret = BeginGPUComputePassNative((SDLGPUCommandBuffer*)pcommandBuffer, (SDLGPUStorageTextureReadWriteBinding*)pstorageTextureBindings, numStorageTextureBindings, (SDLGPUStorageBufferReadWriteBinding*)pstorageBufferBindings, numStorageBufferBindings);
						return ret;
					}
				}
			}
		}

		/// <summary>
		/// Binds a compute pipeline on a command buffer for use in compute dispatch.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUComputePipelineNative(SDLGPUComputePass* computePass, SDLGPUComputePipeline* computePipeline)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, SDLGPUComputePipeline*, void>)funcTable[883])(computePass, computePipeline);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[883])((nint)computePass, (nint)computePipeline);
			#endif
		}

		/// <summary>
		/// Binds a compute pipeline on a command buffer for use in compute dispatch.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputePipeline(SDLGPUComputePass* computePass, SDLGPUComputePipeline* computePipeline)
		{
			BindGPUComputePipelineNative(computePass, computePipeline);
		}

		/// <summary>
		/// Binds a compute pipeline on a command buffer for use in compute dispatch.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputePipeline(ref SDLGPUComputePass computePass, SDLGPUComputePipeline* computePipeline)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				BindGPUComputePipelineNative((SDLGPUComputePass*)pcomputePass, computePipeline);
			}
		}

		/// <summary>
		/// Binds a compute pipeline on a command buffer for use in compute dispatch.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputePipeline(SDLGPUComputePass* computePass, ref SDLGPUComputePipeline computePipeline)
		{
			fixed (SDLGPUComputePipeline* pcomputePipeline = &computePipeline)
			{
				BindGPUComputePipelineNative(computePass, (SDLGPUComputePipeline*)pcomputePipeline);
			}
		}

		/// <summary>
		/// Binds a compute pipeline on a command buffer for use in compute dispatch.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputePipeline(ref SDLGPUComputePass computePass, ref SDLGPUComputePipeline computePipeline)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				fixed (SDLGPUComputePipeline* pcomputePipeline = &computePipeline)
				{
					BindGPUComputePipelineNative((SDLGPUComputePass*)pcomputePass, (SDLGPUComputePipeline*)pcomputePipeline);
				}
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the compute shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUComputeSamplersNative(SDLGPUComputePass* computePass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, uint, SDLGPUTextureSamplerBinding*, uint, void>)funcTable[884])(computePass, firstSlot, textureSamplerBindings, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[884])((nint)computePass, firstSlot, (nint)textureSamplerBindings, numBindings);
			#endif
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the compute shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeSamplers(SDLGPUComputePass* computePass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			BindGPUComputeSamplersNative(computePass, firstSlot, textureSamplerBindings, numBindings);
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the compute shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeSamplers(ref SDLGPUComputePass computePass, uint firstSlot, SDLGPUTextureSamplerBinding* textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				BindGPUComputeSamplersNative((SDLGPUComputePass*)pcomputePass, firstSlot, textureSamplerBindings, numBindings);
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the compute shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeSamplers(SDLGPUComputePass* computePass, uint firstSlot, ref SDLGPUTextureSamplerBinding textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPUTextureSamplerBinding* ptextureSamplerBindings = &textureSamplerBindings)
			{
				BindGPUComputeSamplersNative(computePass, firstSlot, (SDLGPUTextureSamplerBinding*)ptextureSamplerBindings, numBindings);
			}
		}

		/// <summary>
		/// Binds texture-sampler pairs for use on the compute shader.<br/>
		/// The textures must have been created with SDL_GPU_TEXTUREUSAGE_SAMPLER.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeSamplers(ref SDLGPUComputePass computePass, uint firstSlot, ref SDLGPUTextureSamplerBinding textureSamplerBindings, uint numBindings)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				fixed (SDLGPUTextureSamplerBinding* ptextureSamplerBindings = &textureSamplerBindings)
				{
					BindGPUComputeSamplersNative((SDLGPUComputePass*)pcomputePass, firstSlot, (SDLGPUTextureSamplerBinding*)ptextureSamplerBindings, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds storage textures as readonly for use on the compute pipeline.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUComputeStorageTexturesNative(SDLGPUComputePass* computePass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, uint, SDLGPUTexture**, uint, void>)funcTable[885])(computePass, firstSlot, storageTextures, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[885])((nint)computePass, firstSlot, (nint)storageTextures, numBindings);
			#endif
		}

		/// <summary>
		/// Binds storage textures as readonly for use on the compute pipeline.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageTextures(SDLGPUComputePass* computePass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			BindGPUComputeStorageTexturesNative(computePass, firstSlot, storageTextures, numBindings);
		}

		/// <summary>
		/// Binds storage textures as readonly for use on the compute pipeline.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageTextures(ref SDLGPUComputePass computePass, uint firstSlot, SDLGPUTexture** storageTextures, uint numBindings)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				BindGPUComputeStorageTexturesNative((SDLGPUComputePass*)pcomputePass, firstSlot, storageTextures, numBindings);
			}
		}

		/// <summary>
		/// Binds storage textures as readonly for use on the compute pipeline.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageTextures(SDLGPUComputePass* computePass, uint firstSlot, ref SDLGPUTexture* storageTextures, uint numBindings)
		{
			fixed (SDLGPUTexture** pstorageTextures = &storageTextures)
			{
				BindGPUComputeStorageTexturesNative(computePass, firstSlot, (SDLGPUTexture**)pstorageTextures, numBindings);
			}
		}

		/// <summary>
		/// Binds storage textures as readonly for use on the compute pipeline.<br/>
		/// These textures must have been created with<br/>
		/// SDL_GPU_TEXTUREUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageTextures(ref SDLGPUComputePass computePass, uint firstSlot, ref SDLGPUTexture* storageTextures, uint numBindings)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				fixed (SDLGPUTexture** pstorageTextures = &storageTextures)
				{
					BindGPUComputeStorageTexturesNative((SDLGPUComputePass*)pcomputePass, firstSlot, (SDLGPUTexture**)pstorageTextures, numBindings);
				}
			}
		}

		/// <summary>
		/// Binds storage buffers as readonly for use on the compute pipeline.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void BindGPUComputeStorageBuffersNative(SDLGPUComputePass* computePass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, uint, SDLGPUBuffer**, uint, void>)funcTable[886])(computePass, firstSlot, storageBuffers, numBindings);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, nint, uint, void>)funcTable[886])((nint)computePass, firstSlot, (nint)storageBuffers, numBindings);
			#endif
		}

		/// <summary>
		/// Binds storage buffers as readonly for use on the compute pipeline.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageBuffers(SDLGPUComputePass* computePass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			BindGPUComputeStorageBuffersNative(computePass, firstSlot, storageBuffers, numBindings);
		}

		/// <summary>
		/// Binds storage buffers as readonly for use on the compute pipeline.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageBuffers(ref SDLGPUComputePass computePass, uint firstSlot, SDLGPUBuffer** storageBuffers, uint numBindings)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				BindGPUComputeStorageBuffersNative((SDLGPUComputePass*)pcomputePass, firstSlot, storageBuffers, numBindings);
			}
		}

		/// <summary>
		/// Binds storage buffers as readonly for use on the compute pipeline.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageBuffers(SDLGPUComputePass* computePass, uint firstSlot, ref SDLGPUBuffer* storageBuffers, uint numBindings)
		{
			fixed (SDLGPUBuffer** pstorageBuffers = &storageBuffers)
			{
				BindGPUComputeStorageBuffersNative(computePass, firstSlot, (SDLGPUBuffer**)pstorageBuffers, numBindings);
			}
		}

		/// <summary>
		/// Binds storage buffers as readonly for use on the compute pipeline.<br/>
		/// These buffers must have been created with<br/>
		/// SDL_GPU_BUFFERUSAGE_COMPUTE_STORAGE_READ.<br/>
		/// Be sure your shader is set up according to the requirements documented in<br/>
		/// SDL_CreateGPUShader().<br/>
		/// <br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void BindGPUComputeStorageBuffers(ref SDLGPUComputePass computePass, uint firstSlot, ref SDLGPUBuffer* storageBuffers, uint numBindings)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				fixed (SDLGPUBuffer** pstorageBuffers = &storageBuffers)
				{
					BindGPUComputeStorageBuffersNative((SDLGPUComputePass*)pcomputePass, firstSlot, (SDLGPUBuffer**)pstorageBuffers, numBindings);
				}
			}
		}

		/// <summary>
		/// Dispatches compute work.<br/>
		/// You must not call this function before binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void DispatchGPUComputeNative(SDLGPUComputePass* computePass, uint groupcountX, uint groupcountY, uint groupcountZ)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, uint, uint, uint, void>)funcTable[887])(computePass, groupcountX, groupcountY, groupcountZ);
			#else
			((delegate* unmanaged[Cdecl]<nint, uint, uint, uint, void>)funcTable[887])((nint)computePass, groupcountX, groupcountY, groupcountZ);
			#endif
		}

		/// <summary>
		/// Dispatches compute work.<br/>
		/// You must not call this function before binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DispatchGPUCompute(SDLGPUComputePass* computePass, uint groupcountX, uint groupcountY, uint groupcountZ)
		{
			DispatchGPUComputeNative(computePass, groupcountX, groupcountY, groupcountZ);
		}

		/// <summary>
		/// Dispatches compute work.<br/>
		/// You must not call this function before binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DispatchGPUCompute(ref SDLGPUComputePass computePass, uint groupcountX, uint groupcountY, uint groupcountZ)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				DispatchGPUComputeNative((SDLGPUComputePass*)pcomputePass, groupcountX, groupcountY, groupcountZ);
			}
		}

		/// <summary>
		/// Dispatches compute work with parameters set from a buffer.<br/>
		/// The buffer layout should match the layout of<br/>
		/// SDL_GPUIndirectDispatchCommand. You must not call this function before<br/>
		/// binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void DispatchGPUComputeIndirectNative(SDLGPUComputePass* computePass, SDLGPUBuffer* buffer, uint offset)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, SDLGPUBuffer*, uint, void>)funcTable[888])(computePass, buffer, offset);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, uint, void>)funcTable[888])((nint)computePass, (nint)buffer, offset);
			#endif
		}

		/// <summary>
		/// Dispatches compute work with parameters set from a buffer.<br/>
		/// The buffer layout should match the layout of<br/>
		/// SDL_GPUIndirectDispatchCommand. You must not call this function before<br/>
		/// binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DispatchGPUComputeIndirect(SDLGPUComputePass* computePass, SDLGPUBuffer* buffer, uint offset)
		{
			DispatchGPUComputeIndirectNative(computePass, buffer, offset);
		}

		/// <summary>
		/// Dispatches compute work with parameters set from a buffer.<br/>
		/// The buffer layout should match the layout of<br/>
		/// SDL_GPUIndirectDispatchCommand. You must not call this function before<br/>
		/// binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DispatchGPUComputeIndirect(ref SDLGPUComputePass computePass, SDLGPUBuffer* buffer, uint offset)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				DispatchGPUComputeIndirectNative((SDLGPUComputePass*)pcomputePass, buffer, offset);
			}
		}

		/// <summary>
		/// Dispatches compute work with parameters set from a buffer.<br/>
		/// The buffer layout should match the layout of<br/>
		/// SDL_GPUIndirectDispatchCommand. You must not call this function before<br/>
		/// binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DispatchGPUComputeIndirect(SDLGPUComputePass* computePass, ref SDLGPUBuffer buffer, uint offset)
		{
			fixed (SDLGPUBuffer* pbuffer = &buffer)
			{
				DispatchGPUComputeIndirectNative(computePass, (SDLGPUBuffer*)pbuffer, offset);
			}
		}

		/// <summary>
		/// Dispatches compute work with parameters set from a buffer.<br/>
		/// The buffer layout should match the layout of<br/>
		/// SDL_GPUIndirectDispatchCommand. You must not call this function before<br/>
		/// binding a compute pipeline.<br/>
		/// A VERY IMPORTANT NOTE If you dispatch multiple times in a compute pass, and<br/>
		/// the dispatches write to the same resource region as each other, there is no<br/>
		/// guarantee of which order the writes will occur. If the write order matters,<br/>
		/// you MUST end the compute pass and begin another one.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void DispatchGPUComputeIndirect(ref SDLGPUComputePass computePass, ref SDLGPUBuffer buffer, uint offset)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				fixed (SDLGPUBuffer* pbuffer = &buffer)
				{
					DispatchGPUComputeIndirectNative((SDLGPUComputePass*)pcomputePass, (SDLGPUBuffer*)pbuffer, offset);
				}
			}
		}

		/// <summary>
		/// Ends the current compute pass.<br/>
		/// All bound compute state on the command buffer is unset. The compute pass<br/>
		/// handle is now invalid.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void EndGPUComputePassNative(SDLGPUComputePass* computePass)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUComputePass*, void>)funcTable[889])(computePass);
			#else
			((delegate* unmanaged[Cdecl]<nint, void>)funcTable[889])((nint)computePass);
			#endif
		}

		/// <summary>
		/// Ends the current compute pass.<br/>
		/// All bound compute state on the command buffer is unset. The compute pass<br/>
		/// handle is now invalid.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void EndGPUComputePass(SDLGPUComputePass* computePass)
		{
			EndGPUComputePassNative(computePass);
		}

		/// <summary>
		/// Ends the current compute pass.<br/>
		/// All bound compute state on the command buffer is unset. The compute pass<br/>
		/// handle is now invalid.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void EndGPUComputePass(ref SDLGPUComputePass computePass)
		{
			fixed (SDLGPUComputePass* pcomputePass = &computePass)
			{
				EndGPUComputePassNative((SDLGPUComputePass*)pcomputePass);
			}
		}

		/// <summary>
		/// Maps a transfer buffer into application address space.<br/>
		/// You must unmap the transfer buffer before encoding upload commands. The<br/>
		/// memory is owned by the graphics driver - do NOT call SDL_free() on the<br/>
		/// returned pointer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void* MapGPUTransferBufferNative(SDLGPUDevice* device, SDLGPUTransferBuffer* transferBuffer, byte cycle)
		{
			#if NET5_0_OR_GREATER
			return ((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUTransferBuffer*, byte, void*>)funcTable[890])(device, transferBuffer, cycle);
			#else
			return (void*)((delegate* unmanaged[Cdecl]<nint, nint, byte, nint>)funcTable[890])((nint)device, (nint)transferBuffer, cycle);
			#endif
		}

		/// <summary>
		/// Maps a transfer buffer into application address space.<br/>
		/// You must unmap the transfer buffer before encoding upload commands. The<br/>
		/// memory is owned by the graphics driver - do NOT call SDL_free() on the<br/>
		/// returned pointer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void* MapGPUTransferBuffer(SDLGPUDevice* device, SDLGPUTransferBuffer* transferBuffer, bool cycle)
		{
			void* ret = MapGPUTransferBufferNative(device, transferBuffer, cycle ? (byte)1 : (byte)0);
			return ret;
		}

		/// <summary>
		/// Maps a transfer buffer into application address space.<br/>
		/// You must unmap the transfer buffer before encoding upload commands. The<br/>
		/// memory is owned by the graphics driver - do NOT call SDL_free() on the<br/>
		/// returned pointer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void* MapGPUTransferBuffer(ref SDLGPUDevice device, SDLGPUTransferBuffer* transferBuffer, bool cycle)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				void* ret = MapGPUTransferBufferNative((SDLGPUDevice*)pdevice, transferBuffer, cycle ? (byte)1 : (byte)0);
				return ret;
			}
		}

		/// <summary>
		/// Maps a transfer buffer into application address space.<br/>
		/// You must unmap the transfer buffer before encoding upload commands. The<br/>
		/// memory is owned by the graphics driver - do NOT call SDL_free() on the<br/>
		/// returned pointer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void* MapGPUTransferBuffer(SDLGPUDevice* device, ref SDLGPUTransferBuffer transferBuffer, bool cycle)
		{
			fixed (SDLGPUTransferBuffer* ptransferBuffer = &transferBuffer)
			{
				void* ret = MapGPUTransferBufferNative(device, (SDLGPUTransferBuffer*)ptransferBuffer, cycle ? (byte)1 : (byte)0);
				return ret;
			}
		}

		/// <summary>
		/// Maps a transfer buffer into application address space.<br/>
		/// You must unmap the transfer buffer before encoding upload commands. The<br/>
		/// memory is owned by the graphics driver - do NOT call SDL_free() on the<br/>
		/// returned pointer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void* MapGPUTransferBuffer(ref SDLGPUDevice device, ref SDLGPUTransferBuffer transferBuffer, bool cycle)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTransferBuffer* ptransferBuffer = &transferBuffer)
				{
					void* ret = MapGPUTransferBufferNative((SDLGPUDevice*)pdevice, (SDLGPUTransferBuffer*)ptransferBuffer, cycle ? (byte)1 : (byte)0);
					return ret;
				}
			}
		}

		/// <summary>
		/// Unmaps a previously mapped transfer buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void UnmapGPUTransferBufferNative(SDLGPUDevice* device, SDLGPUTransferBuffer* transferBuffer)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUDevice*, SDLGPUTransferBuffer*, void>)funcTable[891])(device, transferBuffer);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, void>)funcTable[891])((nint)device, (nint)transferBuffer);
			#endif
		}

		/// <summary>
		/// Unmaps a previously mapped transfer buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UnmapGPUTransferBuffer(SDLGPUDevice* device, SDLGPUTransferBuffer* transferBuffer)
		{
			UnmapGPUTransferBufferNative(device, transferBuffer);
		}

		/// <summary>
		/// Unmaps a previously mapped transfer buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UnmapGPUTransferBuffer(ref SDLGPUDevice device, SDLGPUTransferBuffer* transferBuffer)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				UnmapGPUTransferBufferNative((SDLGPUDevice*)pdevice, transferBuffer);
			}
		}

		/// <summary>
		/// Unmaps a previously mapped transfer buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UnmapGPUTransferBuffer(SDLGPUDevice* device, ref SDLGPUTransferBuffer transferBuffer)
		{
			fixed (SDLGPUTransferBuffer* ptransferBuffer = &transferBuffer)
			{
				UnmapGPUTransferBufferNative(device, (SDLGPUTransferBuffer*)ptransferBuffer);
			}
		}

		/// <summary>
		/// Unmaps a previously mapped transfer buffer.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UnmapGPUTransferBuffer(ref SDLGPUDevice device, ref SDLGPUTransferBuffer transferBuffer)
		{
			fixed (SDLGPUDevice* pdevice = &device)
			{
				fixed (SDLGPUTransferBuffer* ptransferBuffer = &transferBuffer)
				{
					UnmapGPUTransferBufferNative((SDLGPUDevice*)pdevice, (SDLGPUTransferBuffer*)ptransferBuffer);
				}
			}
		}

		/// <summary>
		/// Begins a copy pass on a command buffer.<br/>
		/// All operations related to copying to or from buffers or textures take place<br/>
		/// inside a copy pass. You must not begin another copy pass, or a render pass<br/>
		/// or compute pass before ending the copy pass.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static SDLGPUCopyPass* BeginGPUCopyPassNative(SDLGPUCommandBuffer* commandBuffer)
		{
			#if NET5_0_OR_GREATER
			return ((delegate* unmanaged[Cdecl]<SDLGPUCommandBuffer*, SDLGPUCopyPass*>)funcTable[892])(commandBuffer);
			#else
			return (SDLGPUCopyPass*)((delegate* unmanaged[Cdecl]<nint, nint>)funcTable[892])((nint)commandBuffer);
			#endif
		}

		/// <summary>
		/// Begins a copy pass on a command buffer.<br/>
		/// All operations related to copying to or from buffers or textures take place<br/>
		/// inside a copy pass. You must not begin another copy pass, or a render pass<br/>
		/// or compute pass before ending the copy pass.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUCopyPass* BeginGPUCopyPass(SDLGPUCommandBuffer* commandBuffer)
		{
			SDLGPUCopyPass* ret = BeginGPUCopyPassNative(commandBuffer);
			return ret;
		}

		/// <summary>
		/// Begins a copy pass on a command buffer.<br/>
		/// All operations related to copying to or from buffers or textures take place<br/>
		/// inside a copy pass. You must not begin another copy pass, or a render pass<br/>
		/// or compute pass before ending the copy pass.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static SDLGPUCopyPass* BeginGPUCopyPass(ref SDLGPUCommandBuffer commandBuffer)
		{
			fixed (SDLGPUCommandBuffer* pcommandBuffer = &commandBuffer)
			{
				SDLGPUCopyPass* ret = BeginGPUCopyPassNative((SDLGPUCommandBuffer*)pcommandBuffer);
				return ret;
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void UploadToGPUTextureNative(SDLGPUCopyPass* copyPass, SDLGPUTextureTransferInfo* source, SDLGPUTextureRegion* destination, byte cycle)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCopyPass*, SDLGPUTextureTransferInfo*, SDLGPUTextureRegion*, byte, void>)funcTable[893])(copyPass, source, destination, cycle);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, nint, byte, void>)funcTable[893])((nint)copyPass, (nint)source, (nint)destination, cycle);
			#endif
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(SDLGPUCopyPass* copyPass, SDLGPUTextureTransferInfo* source, SDLGPUTextureRegion* destination, bool cycle)
		{
			UploadToGPUTextureNative(copyPass, source, destination, cycle ? (byte)1 : (byte)0);
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(ref SDLGPUCopyPass copyPass, SDLGPUTextureTransferInfo* source, SDLGPUTextureRegion* destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				UploadToGPUTextureNative((SDLGPUCopyPass*)pcopyPass, source, destination, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(SDLGPUCopyPass* copyPass, ref SDLGPUTextureTransferInfo source, SDLGPUTextureRegion* destination, bool cycle)
		{
			fixed (SDLGPUTextureTransferInfo* psource = &source)
			{
				UploadToGPUTextureNative(copyPass, (SDLGPUTextureTransferInfo*)psource, destination, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(ref SDLGPUCopyPass copyPass, ref SDLGPUTextureTransferInfo source, SDLGPUTextureRegion* destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTextureTransferInfo* psource = &source)
				{
					UploadToGPUTextureNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUTextureTransferInfo*)psource, destination, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(SDLGPUCopyPass* copyPass, SDLGPUTextureTransferInfo* source, ref SDLGPUTextureRegion destination, bool cycle)
		{
			fixed (SDLGPUTextureRegion* pdestination = &destination)
			{
				UploadToGPUTextureNative(copyPass, source, (SDLGPUTextureRegion*)pdestination, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(ref SDLGPUCopyPass copyPass, SDLGPUTextureTransferInfo* source, ref SDLGPUTextureRegion destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTextureRegion* pdestination = &destination)
				{
					UploadToGPUTextureNative((SDLGPUCopyPass*)pcopyPass, source, (SDLGPUTextureRegion*)pdestination, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(SDLGPUCopyPass* copyPass, ref SDLGPUTextureTransferInfo source, ref SDLGPUTextureRegion destination, bool cycle)
		{
			fixed (SDLGPUTextureTransferInfo* psource = &source)
			{
				fixed (SDLGPUTextureRegion* pdestination = &destination)
				{
					UploadToGPUTextureNative(copyPass, (SDLGPUTextureTransferInfo*)psource, (SDLGPUTextureRegion*)pdestination, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a texture.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// You must align the data in the transfer buffer to a multiple of the texel<br/>
		/// size of the texture format.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUTexture(ref SDLGPUCopyPass copyPass, ref SDLGPUTextureTransferInfo source, ref SDLGPUTextureRegion destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTextureTransferInfo* psource = &source)
				{
					fixed (SDLGPUTextureRegion* pdestination = &destination)
					{
						UploadToGPUTextureNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUTextureTransferInfo*)psource, (SDLGPUTextureRegion*)pdestination, cycle ? (byte)1 : (byte)0);
					}
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void UploadToGPUBufferNative(SDLGPUCopyPass* copyPass, SDLGPUTransferBufferLocation* source, SDLGPUBufferRegion* destination, byte cycle)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCopyPass*, SDLGPUTransferBufferLocation*, SDLGPUBufferRegion*, byte, void>)funcTable[894])(copyPass, source, destination, cycle);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, nint, byte, void>)funcTable[894])((nint)copyPass, (nint)source, (nint)destination, cycle);
			#endif
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(SDLGPUCopyPass* copyPass, SDLGPUTransferBufferLocation* source, SDLGPUBufferRegion* destination, bool cycle)
		{
			UploadToGPUBufferNative(copyPass, source, destination, cycle ? (byte)1 : (byte)0);
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(ref SDLGPUCopyPass copyPass, SDLGPUTransferBufferLocation* source, SDLGPUBufferRegion* destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				UploadToGPUBufferNative((SDLGPUCopyPass*)pcopyPass, source, destination, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(SDLGPUCopyPass* copyPass, ref SDLGPUTransferBufferLocation source, SDLGPUBufferRegion* destination, bool cycle)
		{
			fixed (SDLGPUTransferBufferLocation* psource = &source)
			{
				UploadToGPUBufferNative(copyPass, (SDLGPUTransferBufferLocation*)psource, destination, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(ref SDLGPUCopyPass copyPass, ref SDLGPUTransferBufferLocation source, SDLGPUBufferRegion* destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTransferBufferLocation* psource = &source)
				{
					UploadToGPUBufferNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUTransferBufferLocation*)psource, destination, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(SDLGPUCopyPass* copyPass, SDLGPUTransferBufferLocation* source, ref SDLGPUBufferRegion destination, bool cycle)
		{
			fixed (SDLGPUBufferRegion* pdestination = &destination)
			{
				UploadToGPUBufferNative(copyPass, source, (SDLGPUBufferRegion*)pdestination, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(ref SDLGPUCopyPass copyPass, SDLGPUTransferBufferLocation* source, ref SDLGPUBufferRegion destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUBufferRegion* pdestination = &destination)
				{
					UploadToGPUBufferNative((SDLGPUCopyPass*)pcopyPass, source, (SDLGPUBufferRegion*)pdestination, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(SDLGPUCopyPass* copyPass, ref SDLGPUTransferBufferLocation source, ref SDLGPUBufferRegion destination, bool cycle)
		{
			fixed (SDLGPUTransferBufferLocation* psource = &source)
			{
				fixed (SDLGPUBufferRegion* pdestination = &destination)
				{
					UploadToGPUBufferNative(copyPass, (SDLGPUTransferBufferLocation*)psource, (SDLGPUBufferRegion*)pdestination, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Uploads data from a transfer buffer to a buffer.<br/>
		/// The upload occurs on the GPU timeline. You may assume that the upload has<br/>
		/// finished in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void UploadToGPUBuffer(ref SDLGPUCopyPass copyPass, ref SDLGPUTransferBufferLocation source, ref SDLGPUBufferRegion destination, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTransferBufferLocation* psource = &source)
				{
					fixed (SDLGPUBufferRegion* pdestination = &destination)
					{
						UploadToGPUBufferNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUTransferBufferLocation*)psource, (SDLGPUBufferRegion*)pdestination, cycle ? (byte)1 : (byte)0);
					}
				}
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void CopyGPUTextureToTextureNative(SDLGPUCopyPass* copyPass, SDLGPUTextureLocation* source, SDLGPUTextureLocation* destination, uint w, uint h, uint d, byte cycle)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCopyPass*, SDLGPUTextureLocation*, SDLGPUTextureLocation*, uint, uint, uint, byte, void>)funcTable[895])(copyPass, source, destination, w, h, d, cycle);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, nint, uint, uint, uint, byte, void>)funcTable[895])((nint)copyPass, (nint)source, (nint)destination, w, h, d, cycle);
			#endif
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(SDLGPUCopyPass* copyPass, SDLGPUTextureLocation* source, SDLGPUTextureLocation* destination, uint w, uint h, uint d, bool cycle)
		{
			CopyGPUTextureToTextureNative(copyPass, source, destination, w, h, d, cycle ? (byte)1 : (byte)0);
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(ref SDLGPUCopyPass copyPass, SDLGPUTextureLocation* source, SDLGPUTextureLocation* destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				CopyGPUTextureToTextureNative((SDLGPUCopyPass*)pcopyPass, source, destination, w, h, d, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(SDLGPUCopyPass* copyPass, ref SDLGPUTextureLocation source, SDLGPUTextureLocation* destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUTextureLocation* psource = &source)
			{
				CopyGPUTextureToTextureNative(copyPass, (SDLGPUTextureLocation*)psource, destination, w, h, d, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(ref SDLGPUCopyPass copyPass, ref SDLGPUTextureLocation source, SDLGPUTextureLocation* destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTextureLocation* psource = &source)
				{
					CopyGPUTextureToTextureNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUTextureLocation*)psource, destination, w, h, d, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(SDLGPUCopyPass* copyPass, SDLGPUTextureLocation* source, ref SDLGPUTextureLocation destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUTextureLocation* pdestination = &destination)
			{
				CopyGPUTextureToTextureNative(copyPass, source, (SDLGPUTextureLocation*)pdestination, w, h, d, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(ref SDLGPUCopyPass copyPass, SDLGPUTextureLocation* source, ref SDLGPUTextureLocation destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTextureLocation* pdestination = &destination)
				{
					CopyGPUTextureToTextureNative((SDLGPUCopyPass*)pcopyPass, source, (SDLGPUTextureLocation*)pdestination, w, h, d, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(SDLGPUCopyPass* copyPass, ref SDLGPUTextureLocation source, ref SDLGPUTextureLocation destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUTextureLocation* psource = &source)
			{
				fixed (SDLGPUTextureLocation* pdestination = &destination)
				{
					CopyGPUTextureToTextureNative(copyPass, (SDLGPUTextureLocation*)psource, (SDLGPUTextureLocation*)pdestination, w, h, d, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Performs a texture-to-texture copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUTextureToTexture(ref SDLGPUCopyPass copyPass, ref SDLGPUTextureLocation source, ref SDLGPUTextureLocation destination, uint w, uint h, uint d, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUTextureLocation* psource = &source)
				{
					fixed (SDLGPUTextureLocation* pdestination = &destination)
					{
						CopyGPUTextureToTextureNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUTextureLocation*)psource, (SDLGPUTextureLocation*)pdestination, w, h, d, cycle ? (byte)1 : (byte)0);
					}
				}
			}
		}

		/// <summary>
		/// Performs a buffer-to-buffer copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		[MethodImpl(MethodImplOptions.AggressiveInlining)]
		internal static void CopyGPUBufferToBufferNative(SDLGPUCopyPass* copyPass, SDLGPUBufferLocation* source, SDLGPUBufferLocation* destination, uint size, byte cycle)
		{
			#if NET5_0_OR_GREATER
			((delegate* unmanaged[Cdecl]<SDLGPUCopyPass*, SDLGPUBufferLocation*, SDLGPUBufferLocation*, uint, byte, void>)funcTable[896])(copyPass, source, destination, size, cycle);
			#else
			((delegate* unmanaged[Cdecl]<nint, nint, nint, uint, byte, void>)funcTable[896])((nint)copyPass, (nint)source, (nint)destination, size, cycle);
			#endif
		}

		/// <summary>
		/// Performs a buffer-to-buffer copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUBufferToBuffer(SDLGPUCopyPass* copyPass, SDLGPUBufferLocation* source, SDLGPUBufferLocation* destination, uint size, bool cycle)
		{
			CopyGPUBufferToBufferNative(copyPass, source, destination, size, cycle ? (byte)1 : (byte)0);
		}

		/// <summary>
		/// Performs a buffer-to-buffer copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUBufferToBuffer(ref SDLGPUCopyPass copyPass, SDLGPUBufferLocation* source, SDLGPUBufferLocation* destination, uint size, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				CopyGPUBufferToBufferNative((SDLGPUCopyPass*)pcopyPass, source, destination, size, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Performs a buffer-to-buffer copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUBufferToBuffer(SDLGPUCopyPass* copyPass, ref SDLGPUBufferLocation source, SDLGPUBufferLocation* destination, uint size, bool cycle)
		{
			fixed (SDLGPUBufferLocation* psource = &source)
			{
				CopyGPUBufferToBufferNative(copyPass, (SDLGPUBufferLocation*)psource, destination, size, cycle ? (byte)1 : (byte)0);
			}
		}

		/// <summary>
		/// Performs a buffer-to-buffer copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUBufferToBuffer(ref SDLGPUCopyPass copyPass, ref SDLGPUBufferLocation source, SDLGPUBufferLocation* destination, uint size, bool cycle)
		{
			fixed (SDLGPUCopyPass* pcopyPass = &copyPass)
			{
				fixed (SDLGPUBufferLocation* psource = &source)
				{
					CopyGPUBufferToBufferNative((SDLGPUCopyPass*)pcopyPass, (SDLGPUBufferLocation*)psource, destination, size, cycle ? (byte)1 : (byte)0);
				}
			}
		}

		/// <summary>
		/// Performs a buffer-to-buffer copy.<br/>
		/// This copy occurs on the GPU timeline. You may assume the copy has finished<br/>
		/// in subsequent commands.<br/>
		/// <br/>
		/// <br/>
		/// </summary>
		public static void CopyGPUBufferToBuffer(SDLGPUCopyPass* copyPass, SDLGPUBufferLocation* source, ref SDLGPUBufferLocation destination, uint size, bool cycle)
		{
			fixed (SDLGPUBufferLocation* pdestination = &destination)
			{
				CopyGPUBufferToBufferNative(copyPass, source, (SDLGPUBufferLocation*)pdestination, size, cycle ? (byte)1 : (byte)0);
			}
		}
	}
}
